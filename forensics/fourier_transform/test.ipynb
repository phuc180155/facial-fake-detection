{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fft import *\n",
    "import cv2\n",
    "\n",
    "img_path = \"/mnt/disk1/phucnp/Graduation_Thesis/review/forensics/fourier_transform/test.jpg\"\n",
    "# mode2(img_path)\n",
    "img = cv2.imread(img_path, flags=0)\n",
    "\n",
    "fft = np.fft.fft2(img)\n",
    "mag = np.log(np.abs(fft))\n",
    "ifft = np.real(np.fft.ifft2(mag))\n",
    "print(ifft)\n",
    "print(img)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.title(\"Original image\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(mag, cmap=\"gray\")\n",
    "plt.title('Spectrum image 1')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(ifft, cmap=\"gray\")\n",
    "plt.title('Spectrum image 2')\n",
    "plt.show()\n",
    "\n",
    "# dft_shift = np.fft.fftshift(dft)\n",
    "\n",
    "# # Cộng với 1 lượng epsilon để có thể tính log:\n",
    "# dft_shift += 1e-5\n",
    "\n",
    "# # dft_shift vẫn đang là complex => Ta tìm biên độ của phổ của ảnh:\n",
    "# magnitude_spectrum_1 = cv2.magnitude(dft_shift[:,:,0], dft_shift[:, :, 1])\n",
    "# magnitude_spectrum_2 = 2000 * np.log(cv2.magnitude(dft_shift[:,:,0], dft_shift[:, :, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test (dual) dataset:  80441\n",
      "Test dataset:  80441\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "out spatial tensor([[[[-1.2039e-01, -1.7117e-01, -2.0935e-01, -1.4373e-01],\n",
      "          [-2.5770e-01,  5.9363e-01,  4.2503e-01, -2.7845e-01],\n",
      "          [-2.6346e-01, -1.6691e-01, -2.1873e-01, -2.3634e-01],\n",
      "          [-1.3788e-01, -2.5663e-01, -2.4550e-01, -1.0909e-01]],\n",
      "\n",
      "         [[-6.8217e-02, -1.2960e-01, -1.5599e-01, -9.0285e-02],\n",
      "          [-5.1202e-02,  4.3375e+00,  3.0253e+00, -2.6970e-01],\n",
      "          [-1.7511e-01, -2.6197e-01, -2.7844e-01, -1.5731e-01],\n",
      "          [-7.2113e-02, -2.1681e-01, -2.1030e-01, -7.6695e-02]],\n",
      "\n",
      "         [[-1.5435e-01, -2.3256e-01, -2.5152e-01, -1.6059e-01],\n",
      "          [-2.7683e-01, -3.2005e-02, -8.6295e-02, -2.7277e-01],\n",
      "          [-2.3470e-01,  1.2430e+00,  5.3374e-01, -2.6758e-01],\n",
      "          [-1.4604e-01, -2.5635e-01, -2.3121e-01, -1.0346e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.5036e-01, -2.3878e-01, -2.2643e-01, -2.4513e-01],\n",
      "          [-5.9118e-02, -1.6397e-03, -4.3494e-03, -1.1346e-01],\n",
      "          [-1.8111e-01, -3.1455e-02, -5.5605e-02, -2.4654e-01],\n",
      "          [-7.0656e-02, -2.4285e-01, -2.5997e-01,  1.2587e-01]],\n",
      "\n",
      "         [[-1.3160e-01, -1.9843e-01, -2.2652e-01, -1.6534e-01],\n",
      "          [-2.2434e-01,  1.2543e+00,  9.1113e-01, -2.6182e-01],\n",
      "          [-2.5579e-01, -1.7850e-01, -2.1455e-01, -2.5071e-01],\n",
      "          [-1.3500e-01, -2.5247e-01, -2.4904e-01, -1.3619e-01]],\n",
      "\n",
      "         [[-1.6497e-01, -2.2578e-01, -2.6710e-01, -2.1613e-01],\n",
      "          [-9.8178e-03,  1.1014e+00,  8.9733e-01, -1.4282e-01],\n",
      "          [ 8.3733e-02,  2.6756e+00,  1.5749e+00, -2.4626e-01],\n",
      "          [-2.0286e-01, -2.7504e-01, -2.7781e-01, -1.6704e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.0727e-01, -1.9398e-01, -2.6627e-01, -2.3212e-01],\n",
      "          [-2.7000e-01,  9.0258e-01,  3.8965e-01, -2.7578e-01],\n",
      "          [-2.7842e-01,  4.0170e-01, -8.3919e-03, -2.7394e-01],\n",
      "          [-1.6140e-01, -2.7539e-01, -2.7095e-01, -1.7594e-01]],\n",
      "\n",
      "         [[-6.8686e-02, -1.6923e-01, -2.4381e-01, -1.8313e-01],\n",
      "          [-1.5417e-01,  4.7645e+00,  2.9615e+00, -2.5649e-01],\n",
      "          [-2.3698e-01,  5.5059e-01, -1.2132e-01, -2.3579e-01],\n",
      "          [-8.8363e-02, -2.4293e-01, -2.2517e-01, -1.2518e-01]],\n",
      "\n",
      "         [[-1.7795e-01, -2.6635e-01, -2.7842e-01, -2.4061e-01],\n",
      "          [-2.7572e-01, -8.7992e-02, -1.8597e-01, -2.7411e-01],\n",
      "          [-1.5845e-01,  1.9939e+00,  9.6668e-01, -2.7809e-01],\n",
      "          [-1.8638e-01, -2.7482e-01, -2.7839e-01, -1.8880e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.5893e-01, -1.2022e-01, -1.0662e-01, -2.0998e-01],\n",
      "          [-7.7078e-02, -1.2138e-03, -5.2342e-03, -1.0497e-01],\n",
      "          [-1.1747e-01, -6.9712e-03, -2.5374e-02, -1.7293e-01],\n",
      "          [-2.2576e-01, -1.7907e-01, -2.0028e-01, -2.7118e-01]],\n",
      "\n",
      "         [[-1.1193e-01, -1.8611e-01, -2.5198e-01, -2.3493e-01],\n",
      "          [-2.6976e-01,  1.1725e+00,  6.0476e-01, -2.6189e-01],\n",
      "          [-2.7846e-01,  4.8133e-01,  2.4607e-02, -2.7787e-01],\n",
      "          [-1.5429e-01, -2.7162e-01, -2.6670e-01, -1.9178e-01]],\n",
      "\n",
      "         [[-1.5595e-01, -2.4786e-01, -2.7167e-01, -2.7745e-01],\n",
      "          [-2.3482e-01,  4.2542e-01,  2.2855e-01, -2.1480e-01],\n",
      "          [ 5.1509e-01,  3.9719e+00,  2.6521e+00, -1.0073e-01],\n",
      "          [-2.5521e-01, -5.5729e-02, -1.3988e-01, -2.6191e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.6011e-01, -2.2935e-01, -2.4077e-01, -1.6676e-01],\n",
      "          [-2.7806e-01,  3.0484e-01,  1.0608e-01, -2.7548e-01],\n",
      "          [-1.6894e-01, -7.2115e-02, -1.3591e-01, -1.9537e-01],\n",
      "          [-1.3904e-01, -2.0032e-01, -2.5337e-01, -1.4224e-01]],\n",
      "\n",
      "         [[-9.8851e-02, -1.8684e-01, -1.8523e-01, -1.0750e-01],\n",
      "          [-6.0008e-02,  4.8164e+00,  2.9663e+00, -2.5565e-01],\n",
      "          [-1.3902e-01, -1.4166e-01, -1.3576e-01, -1.4147e-01],\n",
      "          [-9.5875e-02, -2.2829e-01, -2.4915e-01, -1.0765e-01]],\n",
      "\n",
      "         [[-1.9611e-01, -2.6451e-01, -2.5858e-01, -1.6292e-01],\n",
      "          [-2.7839e-01,  9.3062e-02, -1.6277e-01, -2.4270e-01],\n",
      "          [-5.4369e-02,  1.9741e+00,  5.5002e-01, -2.6078e-01],\n",
      "          [-1.7361e-01, -2.7846e-01, -2.6654e-01, -1.4316e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.7663e-01, -1.8405e-01, -1.9820e-01, -2.6718e-01],\n",
      "          [-7.4505e-02, -1.4084e-03, -5.8894e-03, -1.3232e-01],\n",
      "          [-1.2634e-01, -3.0922e-02, -8.3594e-02, -2.5828e-01],\n",
      "          [-2.5274e-01, -1.4658e-01, -1.6564e-01, -2.1187e-01]],\n",
      "\n",
      "         [[-1.6716e-01, -2.3756e-01, -2.5118e-01, -1.9075e-01],\n",
      "          [-2.6088e-01,  7.5280e-01,  6.7598e-01, -2.6749e-01],\n",
      "          [-1.0645e-01, -1.0579e-02, -6.0888e-02, -2.0166e-01],\n",
      "          [-1.3153e-01, -1.1582e-01, -1.9442e-01, -1.5482e-01]],\n",
      "\n",
      "         [[-2.2624e-01, -2.7679e-01, -2.7786e-01, -2.2276e-01],\n",
      "          [-9.6953e-02,  1.0936e+00,  6.7643e-01, -2.5655e-01],\n",
      "          [ 9.0688e-02,  7.1050e-01,  1.3974e-01, -2.7527e-01],\n",
      "          [-2.0996e-01, -2.2919e-01, -2.2513e-01, -1.8539e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.3058e-01, -2.0960e-01, -2.6391e-01, -2.1891e-01],\n",
      "          [-2.5176e-01, -2.4117e-01, -2.6444e-01, -2.7251e-01],\n",
      "          [-1.4900e-01, -4.8326e-02, -1.9807e-01, -2.4035e-01],\n",
      "          [-1.6692e-01, -2.4922e-01, -2.7522e-01, -1.8236e-01]],\n",
      "\n",
      "         [[-7.2305e-02, -1.5084e-01, -2.0704e-01, -1.5817e-01],\n",
      "          [-2.2357e-01, -2.7712e-01, -1.9522e-01, -2.6557e-01],\n",
      "          [-1.1270e-01, -7.5610e-02, -1.9314e-01, -1.9826e-01],\n",
      "          [-1.0911e-01, -2.4335e-01, -2.7822e-01, -1.3992e-01]],\n",
      "\n",
      "         [[-1.7684e-01, -2.5595e-01, -2.7843e-01, -2.3227e-01],\n",
      "          [-2.7182e-01, -2.7327e-01, -2.7181e-01, -2.5115e-01],\n",
      "          [-2.6977e-01, -2.7122e-01, -2.7687e-01, -2.4483e-01],\n",
      "          [-1.7419e-01, -2.6944e-01, -2.7527e-01, -1.8185e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.7784e-01, -1.8900e-01, -1.2967e-01, -2.3669e-01],\n",
      "          [-2.1729e-01, -1.3933e-01, -8.5380e-02, -1.8589e-01],\n",
      "          [-2.2148e-01, -2.1911e-01, -1.5915e-01, -2.3250e-01],\n",
      "          [-2.6120e-01, -1.6364e-01, -1.4112e-01, -2.7501e-01]],\n",
      "\n",
      "         [[-1.4554e-01, -2.2183e-01, -2.6431e-01, -2.2687e-01],\n",
      "          [-2.4973e-01, -2.5258e-01, -2.5337e-01, -2.7681e-01],\n",
      "          [-1.0042e-01, -2.1128e-02, -1.4996e-01, -2.4488e-01],\n",
      "          [-1.5705e-01, -2.0705e-01, -2.7662e-01, -2.0089e-01]],\n",
      "\n",
      "         [[-2.0239e-01, -2.7065e-01, -2.4502e-01, -2.7699e-01],\n",
      "          [-2.5190e-01, -2.7325e-01, -1.5729e-01, -2.7437e-01],\n",
      "          [-2.7003e-01, -1.5576e-01, -2.6761e-01, -2.7846e-01],\n",
      "          [-2.2541e-01, -2.5554e-01, -2.7696e-01, -2.5623e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.6093e-01, -2.6542e-01, -2.7839e-01, -2.4195e-01],\n",
      "          [-2.7764e-01, -3.2816e-03, -8.9805e-02, -2.5908e-01],\n",
      "          [-2.3530e-01, -2.3182e-01, -1.9931e-01, -2.5848e-01],\n",
      "          [-1.7193e-01, -2.7122e-01, -2.6208e-01, -1.7937e-01]],\n",
      "\n",
      "         [[-1.0768e-01, -2.4300e-01, -2.6911e-01, -2.0627e-01],\n",
      "          [-2.7670e-01,  1.7689e+00,  7.2707e-01, -2.3418e-01],\n",
      "          [-1.7481e-01, -2.4509e-01, -1.7084e-01, -2.1538e-01],\n",
      "          [-1.0581e-01, -2.4815e-01, -2.2236e-01, -1.3744e-01]],\n",
      "\n",
      "         [[-2.0383e-01, -2.7846e-01, -2.7237e-01, -2.5321e-01],\n",
      "          [-2.7751e-01,  3.0754e-02, -4.7874e-02, -2.6738e-01],\n",
      "          [-2.7493e-01,  1.7926e-01, -2.2019e-01, -2.7416e-01],\n",
      "          [-1.9135e-01, -2.7834e-01, -2.6753e-01, -1.7041e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.7342e-01, -1.3573e-01, -1.0722e-01, -2.0207e-01],\n",
      "          [-1.3581e-01, -1.0155e-02, -1.8964e-02, -7.3376e-02],\n",
      "          [-1.8905e-01, -6.2302e-02, -1.3536e-01, -1.8723e-01],\n",
      "          [-2.6581e-01, -1.6698e-01, -2.0682e-01, -2.7074e-01]],\n",
      "\n",
      "         [[-1.6631e-01, -2.6118e-01, -2.7536e-01, -2.4645e-01],\n",
      "          [-2.7577e-01,  1.1669e-02, -6.1714e-02, -2.4268e-01],\n",
      "          [-2.0448e-01, -1.5331e-01, -1.5023e-01, -2.5637e-01],\n",
      "          [-1.6316e-01, -2.4821e-01, -2.4340e-01, -1.9295e-01]],\n",
      "\n",
      "         [[-2.2042e-01, -2.6679e-01, -2.0306e-01, -2.7574e-01],\n",
      "          [-1.6856e-01,  6.9488e-01,  6.2620e-01, -2.7944e-03],\n",
      "          [-2.0840e-01,  3.0878e-01, -2.2981e-01, -2.5434e-01],\n",
      "          [-2.5613e-01, -2.5091e-01, -2.7757e-01, -2.4456e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.7760e-01, -2.6719e-01, -2.7287e-01, -2.6478e-01],\n",
      "          [-2.7469e-01, -2.3199e-01, -2.2752e-01, -2.7779e-01],\n",
      "          [-2.6755e-01, -2.7811e-01, -2.7812e-01, -2.7145e-01],\n",
      "          [-1.8122e-01, -2.7749e-01, -2.7709e-01, -1.9083e-01]],\n",
      "\n",
      "         [[-1.0147e-01, -2.1541e-01, -2.7311e-01, -2.3889e-01],\n",
      "          [-2.7392e-01,  4.2927e-01,  1.4562e-02, -2.7841e-01],\n",
      "          [-2.0416e-01, -2.6983e-01, -2.6140e-01, -2.4137e-01],\n",
      "          [-1.1430e-01, -2.6592e-01, -2.6537e-01, -1.5694e-01]],\n",
      "\n",
      "         [[-2.0560e-01, -2.7806e-01, -2.3202e-01, -2.7581e-01],\n",
      "          [-2.6064e-01, -2.7846e-01, -2.7291e-01, -2.7602e-01],\n",
      "          [-2.7650e-01, -1.9049e-01, -2.6399e-01, -2.6950e-01],\n",
      "          [-1.9431e-01, -2.7804e-01, -2.7244e-01, -1.8543e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.7523e-01, -1.2625e-01, -6.9617e-02, -1.3639e-01],\n",
      "          [-1.7881e-01, -4.6448e-02, -6.0025e-02, -1.3412e-01],\n",
      "          [-1.7191e-01, -9.1180e-02, -1.1735e-01, -1.7954e-01],\n",
      "          [-2.7108e-01, -1.6426e-01, -1.7598e-01, -2.7818e-01]],\n",
      "\n",
      "         [[-1.7752e-01, -2.6553e-01, -2.7491e-01, -2.7268e-01],\n",
      "          [-2.7587e-01, -1.8078e-01, -1.7965e-01, -2.7130e-01],\n",
      "          [-2.5561e-01, -2.7194e-01, -2.7785e-01, -2.7642e-01],\n",
      "          [-1.7682e-01, -2.7448e-01, -2.7774e-01, -2.1963e-01]],\n",
      "\n",
      "         [[-2.3474e-01, -2.5623e-01,  8.2138e-03, -1.9466e-01],\n",
      "          [-2.7541e-01, -2.3954e-01, -1.7967e-01, -2.2814e-01],\n",
      "          [-1.6964e-01,  3.0696e-02, -1.7762e-01, -2.5229e-01],\n",
      "          [-2.6254e-01, -2.2643e-01, -2.4663e-01, -2.6527e-01]]]],\n",
      "       grad_fn=<SwishImplementationBackward>)\n",
      "ifreq tensor([[[[-0.0228, -0.0116, -0.0108, -0.0116],\n",
      "          [-0.0107, -0.0105, -0.0109, -0.0117],\n",
      "          [-0.0082, -0.0099, -0.0091, -0.0099],\n",
      "          [-0.0107, -0.0117, -0.0109, -0.0105]],\n",
      "\n",
      "         [[-0.0166, -0.0093, -0.0086, -0.0093],\n",
      "          [-0.0087, -0.0084, -0.0084, -0.0091],\n",
      "          [-0.0068, -0.0079, -0.0071, -0.0079],\n",
      "          [-0.0087, -0.0091, -0.0084, -0.0084]],\n",
      "\n",
      "         [[-0.0194, -0.0094, -0.0142, -0.0094],\n",
      "          [-0.0077, -0.0089, -0.0139, -0.0094],\n",
      "          [-0.0059, -0.0086, -0.0126, -0.0086],\n",
      "          [-0.0077, -0.0094, -0.0139, -0.0089]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0221, -0.0324, -0.0307, -0.0324],\n",
      "          [-0.0325, -0.0351, -0.0332, -0.0337],\n",
      "          [-0.0319, -0.0338, -0.0425, -0.0338],\n",
      "          [-0.0325, -0.0337, -0.0332, -0.0351]],\n",
      "\n",
      "         [[-0.0319, -0.0218, -0.0188, -0.0218],\n",
      "          [-0.0223, -0.0209, -0.0205, -0.0236],\n",
      "          [-0.0175, -0.0200, -0.0170, -0.0200],\n",
      "          [-0.0223, -0.0236, -0.0205, -0.0209]],\n",
      "\n",
      "         [[-0.0325, -0.0311, -0.0261, -0.0311],\n",
      "          [-0.0299, -0.0272, -0.0218, -0.0311],\n",
      "          [-0.0249, -0.0279, -0.0196, -0.0279],\n",
      "          [-0.0299, -0.0311, -0.0218, -0.0272]]],\n",
      "\n",
      "\n",
      "        [[[-0.0314, -0.0275, -0.0257, -0.0275],\n",
      "          [-0.0219, -0.0315, -0.0361, -0.0399],\n",
      "          [-0.0199, -0.0283, -0.0530, -0.0283],\n",
      "          [-0.0219, -0.0399, -0.0361, -0.0315]],\n",
      "\n",
      "         [[-0.0364, -0.0252, -0.0230, -0.0252],\n",
      "          [-0.0295, -0.0294, -0.0281, -0.0288],\n",
      "          [-0.0509, -0.0269, -0.0253, -0.0269],\n",
      "          [-0.0295, -0.0288, -0.0281, -0.0294]],\n",
      "\n",
      "         [[-0.0277, -0.0315, -0.0267, -0.0315],\n",
      "          [-0.0234, -0.0360, -0.0284, -0.0250],\n",
      "          [-0.0207, -0.0267, -0.0327, -0.0267],\n",
      "          [-0.0234, -0.0250, -0.0284, -0.0360]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0229, -0.0294, -0.0301, -0.0294],\n",
      "          [-0.0257, -0.0285, -0.0277, -0.0253],\n",
      "          [-0.0262, -0.0263, -0.0251, -0.0263],\n",
      "          [-0.0257, -0.0253, -0.0277, -0.0285]],\n",
      "\n",
      "         [[-0.0237, -0.0277, -0.0223, -0.0277],\n",
      "          [-0.0233, -0.0248, -0.0246, -0.0249],\n",
      "          [-0.0230, -0.0377, -0.0253, -0.0377],\n",
      "          [-0.0233, -0.0249, -0.0246, -0.0248]],\n",
      "\n",
      "         [[-0.0302, -0.0312, -0.0234, -0.0312],\n",
      "          [-0.0242, -0.0263, -0.0247, -0.0244],\n",
      "          [-0.0199, -0.0241, -0.0233, -0.0241],\n",
      "          [-0.0242, -0.0244, -0.0247, -0.0263]]],\n",
      "\n",
      "\n",
      "        [[[-0.0277, -0.0300, -0.0363, -0.0300],\n",
      "          [-0.0297, -0.0354, -0.0274, -0.0276],\n",
      "          [-0.0212, -0.0282, -0.0219, -0.0282],\n",
      "          [-0.0297, -0.0276, -0.0274, -0.0354]],\n",
      "\n",
      "         [[-0.0326, -0.0324, -0.0319, -0.0324],\n",
      "          [-0.0326, -0.0290, -0.0237, -0.0256],\n",
      "          [-0.0235, -0.0219, -0.0177, -0.0219],\n",
      "          [-0.0326, -0.0256, -0.0237, -0.0290]],\n",
      "\n",
      "         [[-0.0295, -0.0235, -0.0320, -0.0235],\n",
      "          [-0.0277, -0.0280, -0.0305, -0.0282],\n",
      "          [-0.0202, -0.0287, -0.0318, -0.0287],\n",
      "          [-0.0277, -0.0282, -0.0305, -0.0280]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0211, -0.0388, -0.0383, -0.0388],\n",
      "          [-0.0315, -0.0310, -0.0341, -0.0412],\n",
      "          [-0.0300, -0.0311, -0.0259, -0.0311],\n",
      "          [-0.0315, -0.0412, -0.0341, -0.0310]],\n",
      "\n",
      "         [[-0.0211, -0.0348, -0.0262, -0.0348],\n",
      "          [-0.0285, -0.0279, -0.0272, -0.0292],\n",
      "          [-0.0236, -0.0259, -0.0203, -0.0259],\n",
      "          [-0.0285, -0.0292, -0.0272, -0.0279]],\n",
      "\n",
      "         [[-0.0707, -0.0126, -0.0106, -0.0126],\n",
      "          [-0.0140, -0.0129, -0.0123, -0.0142],\n",
      "          [-0.0198, -0.0275, -0.0214, -0.0275],\n",
      "          [-0.0140, -0.0142, -0.0123, -0.0129]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0328, -0.0260, -0.0245, -0.0260],\n",
      "          [-0.0219, -0.0337, -0.0316, -0.0299],\n",
      "          [-0.0194, -0.0286, -0.0374, -0.0286],\n",
      "          [-0.0219, -0.0299, -0.0316, -0.0337]],\n",
      "\n",
      "         [[-0.0372, -0.0279, -0.0251, -0.0279],\n",
      "          [-0.0310, -0.0348, -0.0308, -0.0330],\n",
      "          [-0.0317, -0.0315, -0.0279, -0.0315],\n",
      "          [-0.0310, -0.0330, -0.0308, -0.0348]],\n",
      "\n",
      "         [[-0.0266, -0.0242, -0.0223, -0.0242],\n",
      "          [-0.0248, -0.0239, -0.0277, -0.0277],\n",
      "          [-0.0203, -0.0265, -0.0265, -0.0265],\n",
      "          [-0.0248, -0.0277, -0.0277, -0.0239]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0228, -0.0286, -0.0291, -0.0286],\n",
      "          [-0.0266, -0.0261, -0.0283, -0.0236],\n",
      "          [-0.0300, -0.0246, -0.0264, -0.0246],\n",
      "          [-0.0266, -0.0236, -0.0283, -0.0261]],\n",
      "\n",
      "         [[-0.0246, -0.0235, -0.0207, -0.0235],\n",
      "          [-0.0248, -0.0291, -0.0251, -0.0267],\n",
      "          [-0.0216, -0.0269, -0.0231, -0.0269],\n",
      "          [-0.0248, -0.0267, -0.0251, -0.0291]],\n",
      "\n",
      "         [[-0.0299, -0.0264, -0.0208, -0.0264],\n",
      "          [-0.0225, -0.0268, -0.0286, -0.0346],\n",
      "          [-0.0195, -0.0243, -0.0236, -0.0243],\n",
      "          [-0.0225, -0.0346, -0.0286, -0.0268]]],\n",
      "\n",
      "\n",
      "        [[[-0.0286, -0.0323, -0.0218, -0.0323],\n",
      "          [-0.0290, -0.0292, -0.0238, -0.0312],\n",
      "          [-0.0265, -0.0282, -0.0197, -0.0282],\n",
      "          [-0.0290, -0.0312, -0.0238, -0.0292]],\n",
      "\n",
      "         [[-0.0563, -0.0167, -0.0138, -0.0167],\n",
      "          [-0.0201, -0.0156, -0.0137, -0.0162],\n",
      "          [-0.0162, -0.0147, -0.0114, -0.0147],\n",
      "          [-0.0201, -0.0162, -0.0137, -0.0156]],\n",
      "\n",
      "         [[-0.0511, -0.0135, -0.0136, -0.0135],\n",
      "          [-0.0146, -0.0130, -0.0135, -0.0134],\n",
      "          [-0.0113, -0.0126, -0.0116, -0.0126],\n",
      "          [-0.0146, -0.0134, -0.0135, -0.0130]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0219, -0.0279, -0.0286, -0.0279],\n",
      "          [-0.0274, -0.0297, -0.0306, -0.0292],\n",
      "          [-0.0293, -0.0296, -0.0316, -0.0296],\n",
      "          [-0.0274, -0.0292, -0.0306, -0.0297]],\n",
      "\n",
      "         [[-0.0206, -0.0355, -0.0260, -0.0355],\n",
      "          [-0.0293, -0.0335, -0.0303, -0.0325],\n",
      "          [-0.0336, -0.0312, -0.0380, -0.0312],\n",
      "          [-0.0293, -0.0325, -0.0303, -0.0335]],\n",
      "\n",
      "         [[-0.0269, -0.0350, -0.0267, -0.0350],\n",
      "          [-0.0317, -0.0328, -0.0345, -0.0328],\n",
      "          [-0.0269, -0.0299, -0.0281, -0.0299],\n",
      "          [-0.0317, -0.0328, -0.0345, -0.0328]]],\n",
      "\n",
      "\n",
      "        [[[-0.0305, -0.0284, -0.0275, -0.0284],\n",
      "          [-0.0233, -0.0393, -0.0394, -0.0364],\n",
      "          [-0.0199, -0.0313, -0.0390, -0.0313],\n",
      "          [-0.0233, -0.0364, -0.0394, -0.0393]],\n",
      "\n",
      "         [[-0.0306, -0.0275, -0.0252, -0.0275],\n",
      "          [-0.0328, -0.0315, -0.0295, -0.0316],\n",
      "          [-0.0361, -0.0306, -0.0285, -0.0306],\n",
      "          [-0.0328, -0.0316, -0.0295, -0.0315]],\n",
      "\n",
      "         [[-0.0231, -0.0323, -0.0277, -0.0323],\n",
      "          [-0.0256, -0.0267, -0.0293, -0.0298],\n",
      "          [-0.0225, -0.0232, -0.0230, -0.0232],\n",
      "          [-0.0256, -0.0298, -0.0293, -0.0267]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0209, -0.0306, -0.0274, -0.0306],\n",
      "          [-0.0317, -0.0271, -0.0282, -0.0268],\n",
      "          [-0.0345, -0.0258, -0.0262, -0.0258],\n",
      "          [-0.0317, -0.0268, -0.0282, -0.0271]],\n",
      "\n",
      "         [[-0.0238, -0.0230, -0.0221, -0.0230],\n",
      "          [-0.0242, -0.0282, -0.0272, -0.0263],\n",
      "          [-0.0213, -0.0266, -0.0237, -0.0266],\n",
      "          [-0.0242, -0.0263, -0.0272, -0.0282]],\n",
      "\n",
      "         [[-0.0325, -0.0200, -0.0199, -0.0200],\n",
      "          [-0.0173, -0.0171, -0.0173, -0.0181],\n",
      "          [-0.0272, -0.0165, -0.0153, -0.0165],\n",
      "          [-0.0173, -0.0181, -0.0173, -0.0171]]]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/mnt/disk1/phucnp/Graduation_Thesis/review/forensics/dl_technique\")\n",
    "from model.vision_transformer.dual_efficient_vit import DualEfficientViT\n",
    "from dataloader.gen_dataloader import generate_test_dataloader_dual_stream\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device('cpu')\n",
    "test_dir = \"/mnt/disk1/phucnp/Dataset/dfdc/image/test\"\n",
    "test_loader = generate_test_dataloader_dual_stream(test_dir, 128, 32, 4)\n",
    "it = iter(test_loader)\n",
    "inputs, ffts, labels = it.next()\n",
    "inputs, ffts, labels = inputs.float().to(device), ffts.float().to(device), labels.float().to(device)\n",
    "\n",
    "model = DualEfficientViT(image_size=128, heads=3, depth=4, patch_size=2, freeze=0, version=\"cross_attention-freq-add\", weight=0.8)\n",
    "\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(\"/mnt/disk1/phucnp/Graduation_Thesis/review/forensics/dl_technique/checkpoint/dfdc/dual_efficient_vit/v_cross_attention-freq-add_w_0.8_lr_0.0003_patch_2_h_3_d_4_es_none_loss_bce_freeze_0/step/best_test_acc_0.726794.pt\", map_location=device))\n",
    "\n",
    "model.eval()\n",
    "out_spa = model.spatial_extractor.extract_features(inputs)\n",
    "out_fft = model.freq_extractor.extract_features(ffts)\n",
    "ifreq_features =  F.normalize(torch.log(torch.abs(torch.fft.ifft2(torch.fft.ifftshift(out_fft)) + 1e-5)))\n",
    "print(\"out spatial\", out_spa)\n",
    "# print(\"out fft\", out_fft)\n",
    "print(\"ifreq\", ifreq_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from einops import rearrange\n",
    "\n",
    "import sys\n",
    "from model.backbone.efficient_net.model import EfficientNet\n",
    "\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re, math\n",
    "from model.vision_transformer.vit import Transformer\n",
    "\n",
    "\n",
    "class DualEfficientViT(nn.Module):\n",
    "    def __init__(self, channels=1280,\\\n",
    "                 image_size=224,patch_size=7,num_classes=1,dim=1024,\\\n",
    "                 depth=6,heads=8,mlp_dim=2048,\\\n",
    "                 emb_dim=32, dim_head=64,dropout=0.15,emb_dropout=0.15,version=\"cross_attention-spatial-cat\",weight=0.5,freeze=0, pool='cls'):  \n",
    "        super(DualEfficientViT, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dim_head = dim_head\n",
    "        self.dropout_value = dropout\n",
    "        self.emb_dropout = emb_dropout\n",
    "        self.pool = pool\n",
    "        \n",
    "        self.features_size = {\n",
    "            128: (4, 4),\n",
    "            224: (7, 7),\n",
    "            256: (8, 8)\n",
    "        }\n",
    "        \n",
    "        # \"cross_attention-spatial-cat\": sử dụng cross-attention, cat với spatial vectors output\n",
    "        # \"cross_attention-spatial-add\": sử dụng cross-attention, add với spatial vectors output\n",
    "        # \"cross_attention-freq-cat\": sử dụng cross-attention, cat với freq vectors\n",
    "        # \"cross_attention-freq-add\": sử dụng cross-attention, add với freq vectors\n",
    "        # \"merge-add\": cộng thẳng 2 vectors spatial và freq, có weight: spatial + weight*freq\n",
    "        # \"merge-cat\": cat thẳng 2 vectors spatial và freq, có weight: spatial + weight*freq\n",
    "        self.version = version\n",
    "        self.weight = weight\n",
    "\n",
    "        self.spatial_extractor = self.get_feature_extractor(freeze=freeze, num_classes=num_classes, in_channels=3)   # efficient_net-b0, return shape (1280, 8, 8) or (1280, 7, 7)\n",
    "        self.freq_extractor = self.get_feature_extractor(freeze=freeze, num_classes=num_classes, in_channels=1)\n",
    "\n",
    "        ############################# Xét 2 stream hiện tại là như nhau\n",
    "        # Kích thước của 1 patch\n",
    "        self.patch_size = patch_size\n",
    "    \n",
    "        # Số lượng patches\n",
    "        self.num_patches = int((self.features_size[image_size][0] * self.features_size[image_size][1]) / (self.patch_size * self.patch_size))\n",
    "        # Patch_dim = P^2 * C\n",
    "        self.patch_dim = channels * (self.patch_size ** 2)\n",
    "\n",
    "        # print(\"Num patches: \", self.num_patches)\n",
    "        # print(\"Patch dim: \", self.patch_dim)\n",
    "\n",
    "        # Embed vị trí cho từng patch\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches+1, self.dim))\n",
    "        # self.pos_embedding_1 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "        # self.pos_embedding_2 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "        # self.pos_embedding_3 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "\n",
    "        # Đưa flatten vector của feature maps về chiều cố định của vector trong transformer.\n",
    "        # self.patch_to_embedding_1 = nn.Linear(self.patch_dim, self.dim)\n",
    "        # self.patch_to_embedding_2 = nn.Linear(self.patch_dim, self.dim)\n",
    "\n",
    "        # Giảm chiều vector sau concat 2*patch_dim về D:\n",
    "        self.patch_to_embedding_cat = nn.Linear(2*self.patch_dim, self.dim)\n",
    "        self.patch_to_embedding_add = nn.Linear(self.patch_dim, self.dim)\n",
    "\n",
    "        # Thêm 1 embedding vector cho classify token:\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(self.emb_dropout)\n",
    "        self.transformer = Transformer(self.dim, self.depth, self.heads, self.dim_head, self.mlp_dim, self.dropout_value)\n",
    "\n",
    "        self.to_cls_token = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(self.dim, self.mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.mlp_dim, self.num_classes)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def get_feature_extractor(self, architecture=\"efficient_net\", freeze=0, pretrained=\"\", num_classes=1, in_channels=3):\n",
    "        extractor = None\n",
    "        if architecture == \"efficient_net\":\n",
    "            if pretrained == \"\":\n",
    "                extractor = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes,in_channels = in_channels)\n",
    "            else:\n",
    "                extractor = EfficientNet.from_pretrained('efficientnet-b7', num_classes=num_classes,in_channels = in_channels)\n",
    "                # Load checkpoint\n",
    "                checkpoint = torch.load(pretrained, map_location=\"cpu\")\n",
    "                state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
    "                # Load weights\n",
    "                extractor.load_state_dict({re.sub(\"^module.\", \"\", k): v for k, v in state_dict.items()}, strict=False)\n",
    "\n",
    "            if freeze:\n",
    "            # Freeze the first (num_blocks - 3) blocks and unfreeze the rest \n",
    "                for i in range(0, len(extractor._blocks)):\n",
    "                    for index, param in enumerate(extractor._blocks[i].parameters()):\n",
    "                        if i >= len(extractor._blocks) - 3:\n",
    "                            param.requires_grad = True\n",
    "                        else:\n",
    "                            param.requires_grad = False\n",
    "        elif architecture == \"xception_net\":\n",
    "            pass\n",
    "        return extractor\n",
    "\n",
    "    \"\"\"\n",
    "        Get from torch.nn.MultiheadAttention\n",
    "        scale-dot: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L4966\n",
    "        multi-head: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L5059\n",
    "    \"\"\"\n",
    "    def scale_dot(self, q, k, v, attn_mask=None, dropout_p=0):\n",
    "        B, Nt, E = q.shape\n",
    "        q = q / math.sqrt(E)\n",
    "        # (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\n",
    "        attn = torch.bmm(q, k.transpose(-2, -1))\n",
    "        if attn_mask is not None:\n",
    "            attn += attn_mask\n",
    "        attn = torch.nn.functional.softmax(attn, dim=-1)\n",
    "        if dropout_p > 0.0:\n",
    "            attn = torch.nn.functional.dropout(attn, p=dropout_p)\n",
    "        # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\n",
    "        output = torch.bmm(attn, v)\n",
    "        return output, attn\n",
    "\n",
    "    def cross_attention(self, spatials, ifreqs):\n",
    "        \"\"\"\n",
    "            spatials: (B, N, D) --> Query,\n",
    "            freqs: (B, N, D) --> Key\n",
    "            output: \n",
    "        \"\"\"\n",
    "        emb_dim = spatials.shape[2]\n",
    "        assert emb_dim == ifreqs.shape[2]\n",
    "        attn_outputs, attn_weights = self.scale_dot(spatials, ifreqs, ifreqs)\n",
    "        return attn_outputs, attn_weights\n",
    "\n",
    "\n",
    "    def forward(self, spatial_imgs, frequency_imgs):\n",
    "        p = self.patch_size\n",
    "        # Extract features\n",
    "        spatial_features = self.spatial_extractor.extract_features(spatial_imgs)                 # shape (batchsize, 1280, 8, 8)\n",
    "        freq_features = self.freq_extractor.extract_features(frequency_imgs)                     # shape (batchsize, 1280, 8, 8)conda\n",
    "        ifreq_features = torch.log(torch.abs(torch.fft.ifft2(torch.fft.ifftshift(freq_features))) + 1e-10)  # Hơi ảo???\n",
    "        # print(ifreq_features.shape)\n",
    "        # assert(ifreq_features.shape == freq_features.shape)\n",
    "        # print(\"Features shape: \", spatial_features.shape, freq_features.shape)\n",
    "\n",
    "        # Flatten to vector:\n",
    "        spatial_vectors = rearrange(spatial_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        freq_vectors = rearrange(freq_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        ifreq_vectors = rearrange(ifreq_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "\n",
    "        assert self.patch_dim == spatial_vectors.shape[2]\n",
    "        assert self.patch_dim == freq_vectors.shape[2]\n",
    "\n",
    "        embed = None\n",
    "        \n",
    "        if \"cross_attention\" in self.version:          # Merge using cross-attention  \n",
    "            ########## Patch embedding and add position embedding to each domain:\n",
    "            # spatial_vectors = self.patch_to_embedding_1(spatial_vectors)\n",
    "            # spatial_vectors += self.pos_embedding_1\n",
    "\n",
    "            # freq_vectors = self.patch_to_embedding_2(freq_vectors)\n",
    "            # freq_vectors += self.pos_embedding_2\n",
    "\n",
    "            # ifreq_vectors = self.patch_to_embedding_2(ifreq_vectors)\n",
    "            # ifreq_vectors += self.pos_embedding_2  \n",
    "            # print(\"Step 2 shape: \", spatial_vectors.shape, freq_vectors.shape)  # (batchsize, num_patches, D)\n",
    "            ##########\n",
    "        \n",
    "            # Cal attn weight between ifreq and spatial vectors:\n",
    "            # Cross-attention (spatial-decoder, ifreq-encoder)\n",
    "            attn_outputs, attn_weights = self.cross_attention(spatial_vectors, ifreq_vectors)     # Shape: (), (batchsize, num_patches, num_patches)\n",
    "            if \"freq\" in self.version:          # Get attention in frequency domain:\n",
    "                out_attn = torch.bmm(attn_weights, freq_vectors)\n",
    "            elif \"spatial\" in self.version:     # Get attention in spatial domain:\n",
    "                out_attn = torch.bmm(attn_weights, ifreq_vectors)\n",
    "                ### Check correct bmm:\n",
    "                # print(torch.eq(attn_outputs, out_attn))\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # Concat or add and linear\n",
    "            # print(\"Spatial vectors: \", spatial_vectors.shape)\n",
    "            # print(spatial_vectors)\n",
    "            # print(\"Output attention: \", out_attn.shape)\n",
    "            # print(out_attn)\n",
    "            if \"add\" in self.version:\n",
    "                out = torch.add(spatial_vectors, self.weight * out_attn)\n",
    "                # print(\"Out\", out)\n",
    "                embed = self.patch_to_embedding_add(out)                 # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            elif \"cat\" in self.version:\n",
    "                out = torch.cat([spatial_vectors, self.weight * out_attn], dim=2)\n",
    "                embed = self.patch_to_embedding_cat(out)                 # Shape: (batchsize, num_patches, 2*patch_dim) => (batchsize, num_patches, dim)\n",
    "            else:\n",
    "                pass\n",
    "        else:   # Merge directly\n",
    "            if \"add\" in self.version:\n",
    "                out = torch.add(spatial_vectors, self.weight * freq_vectors)\n",
    "                embed = self.patch_to_embedding_add(out)                # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            elif \"cat\" in self.version:\n",
    "                out = torch.cat([spatial_vectors, self.weight * freq_vectors], dim=2)\n",
    "                embed = self.patch_to_embedding_cat(out)                # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        # print(\"Embeded shape: \", embed.shape)\n",
    "\n",
    "        # Expand classify token to batchsize and add to patch embeddings:\n",
    "        cls_tokens = self.cls_token.expand(embed.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, embed), dim=1)   # (batchsize, num_patches+1, dim)\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.to_cls_token(x.mean(dim = 1) if self.pool == 'mean' else x[:, 0])\n",
    "        x = self.mlp_head(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x = torch.ones(32, 3, 256, 256)\n",
    "    y = torch.ones(32, 1, 256, 256)\n",
    "    model_ = DualEfficientViT(image_size=256, patch_size=2)\n",
    "    out = model_(x, y)\n",
    "    print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c99a311309def2227702d5f2c551c7a298c8ef3054a27c10387f667008a8451"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('phucnp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
