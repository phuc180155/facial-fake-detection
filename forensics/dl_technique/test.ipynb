{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiotlab/anaconda3/envs/phucnp/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3666],\n",
       "        [0.3717],\n",
       "        [0.3337]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.vision_transformer.dual_efficient_vit import DualEfficientViT\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "torch.manual_seed(0)\n",
    "image = torch.randn((3, 3, 128, 128))\n",
    "freq = torch.randn((3, 1, 128, 128))\n",
    "\n",
    "model = DualEfficientViT(image_size=128, patch_size=1, version=\"cross_attention-freq-add\", weight=0.5)\n",
    "\n",
    "out = model(image, freq)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST\n",
      "target =  tensor([1., 0., 0., 0., 1., 0.])\n",
      "pred =  tensor([0.3251, 0.0902, 0.3936, 0.6069, 0.1743, 0.4743])\n",
      "tensor([1.1237, 0.0945, 0.5003, 0.9336, 1.7472, 0.6431])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8404)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class WeightedBinaryCrossEntropy(nn.Module):\n",
    "    def __init__(self, weights=None, eps=1e-7):\n",
    "        super(WeightedBinaryCrossEntropy, self).__init__()\n",
    "        self.weights = weights      # [class_0, class_1]\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, pred, target):    # target: (batch_size, ), pred: (batch_size, )\n",
    "        pred = pred.clamp(self.eps, 1-self.eps)\n",
    "        if self.weights is not None:\n",
    "            loss = self.weights[1] * (target * torch.log(pred)) + self.weights[0] * ((1- target) * torch.log(1-pred))\n",
    "        else:\n",
    "            loss = target * torch.log(pred) + (1 - target) * torch.log(1-pred)\n",
    "        return torch.mean(torch.neg(loss))\n",
    "    \n",
    "# class_0, class_1 = 1, 4\n",
    "# total = class_0 + class_1\n",
    "# weights = [0.5 * total/class_0, 0.5 * total/class_1]\n",
    "# wbce = WeightedBinaryCrossEntropy(weights=weights)\n",
    "\n",
    "# print(weights)\n",
    "# target = torch.tensor([0.0, 1.0, 1.0, 1.0, 1.0], dtype=torch.float32)\n",
    "# pred = torch.tensor([0.4, 0.6, 1.0, 1.0, 0.6])\n",
    "\n",
    "# loss = wbce(target, pred)\n",
    "# print(loss)\n",
    "\n",
    "print(\"TEST\")\n",
    "bce = nn.BCELoss(reduction='none')\n",
    "wbce = WeightedBinaryCrossEntropy(weights=[1,1])\n",
    "pred = torch.rand((6))\n",
    "tar = torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float32)\n",
    "print(\"target = \", tar )\n",
    "print(\"pred = \", pred)\n",
    "print(bce(pred, tar))\n",
    "wbce(pred, tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weights=None, gamma=0, eps=1e-7):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.weights = weights      # [class_0, class_1] vÃ  class_0 + class_1 = 1\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.clamp(self.eps, 1 - self.eps)\n",
    "        if self.weights is not None:\n",
    "            loss = self.weights[1] * ((1 - pred) ** self.gamma) * target * torch.log(pred) + self.weights[0] * (pred ** self.gamma) * (1 - target) * torch.log(1 - pred)\n",
    "        else:\n",
    "            loss = ((1 - pred) * self.gamma) * target * torch.log(pred) + (pred ** self.gamma) * (1 - target) * torch.log(1 - pred)\n",
    "        return torch.mean(torch.neg(loss))\n",
    "\n",
    "print(\"Test loss\")\n",
    "torch.manual_seed(4)\n",
    "# target = torch.tensor([0.0, 1.0, 1.0, 1.0, 1.0], dtype=torch.float32)\n",
    "# pred = torch.tensor([0.2, 0.8, 0.8, 0.8, 0.8])\n",
    "\n",
    "batch = 4\n",
    "pred = torch.rand((batch, ))\n",
    "tar = torch.randint(0, 2, (batch, ))\n",
    "num_0 = (tar > 0).to(torch.int32).sum().item()\n",
    "num_1 = (tar == 0).to(torch.int32).sum(0).item()\n",
    "\n",
    "print(\"Pred: \", pred)\n",
    "print(\"Target: \", tar.shape)\n",
    "print(\"Numner: \", num_0, num_1)\n",
    "weights = [num_1 / (num_0 + num_1), num_0 / (num_0 + num_1)]\n",
    "print(weights)\n",
    "\n",
    "fl = FocalLoss(weights=weights, gamma=0.5)\n",
    "fcloss = fl(pred, tar)\n",
    "\n",
    "wbce = WeightedBinaryCrossEntropy(weights=weights)\n",
    "wbceloss = wbce(pred, tar)\n",
    "\n",
    "print(\"Focal loss: \", fcloss)\n",
    "print(\"WBCE loss: \",wbceloss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Extractor:  EfficientNet(\n",
      "  (_conv_stem): Conv2dStaticSamePadding(\n",
      "    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
      "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
      "  )\n",
      "  (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "  (_blocks): ModuleList(\n",
      "    (0): MBConvBlock(\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (1): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (2): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (3): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (4): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (5): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (6): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (7): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (8): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (9): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (10): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (11): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (12): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (13): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (14): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (15): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "  )\n",
      "  (_conv_head): Conv2dStaticSamePadding(\n",
      "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "    (static_padding): Identity()\n",
      "  )\n",
      "  (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
      "  (_dropout): Dropout(p=0.2, inplace=False)\n",
      "  (_fc): Linear(in_features=1280, out_features=1, bias=True)\n",
      "  (_swish): MemoryEfficientSwish()\n",
      ")\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Extractor:  EfficientNet(\n",
      "  (_conv_stem): Conv2dStaticSamePadding(\n",
      "    1, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
      "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
      "  )\n",
      "  (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "  (_blocks): ModuleList(\n",
      "    (0): MBConvBlock(\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (1): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (2): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (3): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (4): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (5): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (6): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (7): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (8): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (9): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (10): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (11): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (12): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (13): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (14): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (15): MBConvBlock(\n",
      "      (_expand_conv): Conv2dStaticSamePadding(\n",
      "        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
      "        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
      "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
      "      )\n",
      "      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2dStaticSamePadding(\n",
      "        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_se_expand): Conv2dStaticSamePadding(\n",
      "        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_project_conv): Conv2dStaticSamePadding(\n",
      "        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (static_padding): Identity()\n",
      "      )\n",
      "      (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "  )\n",
      "  (_conv_head): Conv2dStaticSamePadding(\n",
      "    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "    (static_padding): Identity()\n",
      "  )\n",
      "  (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
      "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
      "  (_dropout): Dropout(p=0.2, inplace=False)\n",
      "  (_fc): Linear(in_features=1280, out_features=1, bias=True)\n",
      "  (_swish): MemoryEfficientSwish()\n",
      ")\n",
      "torch.Size([32, 1])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         ZeroPad2d-1          [-1, 3, 257, 257]               0\n",
      "Conv2dStaticSamePadding-2         [-1, 32, 128, 128]             864\n",
      "       BatchNorm2d-3         [-1, 32, 128, 128]              64\n",
      "MemoryEfficientSwish-4         [-1, 32, 128, 128]               0\n",
      "         ZeroPad2d-5         [-1, 32, 130, 130]               0\n",
      "Conv2dStaticSamePadding-6         [-1, 32, 128, 128]             288\n",
      "       BatchNorm2d-7         [-1, 32, 128, 128]              64\n",
      "MemoryEfficientSwish-8         [-1, 32, 128, 128]               0\n",
      "          Identity-9             [-1, 32, 1, 1]               0\n",
      "Conv2dStaticSamePadding-10              [-1, 8, 1, 1]             264\n",
      "MemoryEfficientSwish-11              [-1, 8, 1, 1]               0\n",
      "         Identity-12              [-1, 8, 1, 1]               0\n",
      "Conv2dStaticSamePadding-13             [-1, 32, 1, 1]             288\n",
      "         Identity-14         [-1, 32, 128, 128]               0\n",
      "Conv2dStaticSamePadding-15         [-1, 16, 128, 128]             512\n",
      "      BatchNorm2d-16         [-1, 16, 128, 128]              32\n",
      "      MBConvBlock-17         [-1, 16, 128, 128]               0\n",
      "         Identity-18         [-1, 16, 128, 128]               0\n",
      "Conv2dStaticSamePadding-19         [-1, 96, 128, 128]           1,536\n",
      "      BatchNorm2d-20         [-1, 96, 128, 128]             192\n",
      "MemoryEfficientSwish-21         [-1, 96, 128, 128]               0\n",
      "        ZeroPad2d-22         [-1, 96, 129, 129]               0\n",
      "Conv2dStaticSamePadding-23           [-1, 96, 64, 64]             864\n",
      "      BatchNorm2d-24           [-1, 96, 64, 64]             192\n",
      "MemoryEfficientSwish-25           [-1, 96, 64, 64]               0\n",
      "         Identity-26             [-1, 96, 1, 1]               0\n",
      "Conv2dStaticSamePadding-27              [-1, 4, 1, 1]             388\n",
      "MemoryEfficientSwish-28              [-1, 4, 1, 1]               0\n",
      "         Identity-29              [-1, 4, 1, 1]               0\n",
      "Conv2dStaticSamePadding-30             [-1, 96, 1, 1]             480\n",
      "         Identity-31           [-1, 96, 64, 64]               0\n",
      "Conv2dStaticSamePadding-32           [-1, 24, 64, 64]           2,304\n",
      "      BatchNorm2d-33           [-1, 24, 64, 64]              48\n",
      "      MBConvBlock-34           [-1, 24, 64, 64]               0\n",
      "         Identity-35           [-1, 24, 64, 64]               0\n",
      "Conv2dStaticSamePadding-36          [-1, 144, 64, 64]           3,456\n",
      "      BatchNorm2d-37          [-1, 144, 64, 64]             288\n",
      "MemoryEfficientSwish-38          [-1, 144, 64, 64]               0\n",
      "        ZeroPad2d-39          [-1, 144, 66, 66]               0\n",
      "Conv2dStaticSamePadding-40          [-1, 144, 64, 64]           1,296\n",
      "      BatchNorm2d-41          [-1, 144, 64, 64]             288\n",
      "MemoryEfficientSwish-42          [-1, 144, 64, 64]               0\n",
      "         Identity-43            [-1, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-44              [-1, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-45              [-1, 6, 1, 1]               0\n",
      "         Identity-46              [-1, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-47            [-1, 144, 1, 1]           1,008\n",
      "         Identity-48          [-1, 144, 64, 64]               0\n",
      "Conv2dStaticSamePadding-49           [-1, 24, 64, 64]           3,456\n",
      "      BatchNorm2d-50           [-1, 24, 64, 64]              48\n",
      "      MBConvBlock-51           [-1, 24, 64, 64]               0\n",
      "         Identity-52           [-1, 24, 64, 64]               0\n",
      "Conv2dStaticSamePadding-53          [-1, 144, 64, 64]           3,456\n",
      "      BatchNorm2d-54          [-1, 144, 64, 64]             288\n",
      "MemoryEfficientSwish-55          [-1, 144, 64, 64]               0\n",
      "        ZeroPad2d-56          [-1, 144, 67, 67]               0\n",
      "Conv2dStaticSamePadding-57          [-1, 144, 32, 32]           3,600\n",
      "      BatchNorm2d-58          [-1, 144, 32, 32]             288\n",
      "MemoryEfficientSwish-59          [-1, 144, 32, 32]               0\n",
      "         Identity-60            [-1, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-61              [-1, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-62              [-1, 6, 1, 1]               0\n",
      "         Identity-63              [-1, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-64            [-1, 144, 1, 1]           1,008\n",
      "         Identity-65          [-1, 144, 32, 32]               0\n",
      "Conv2dStaticSamePadding-66           [-1, 40, 32, 32]           5,760\n",
      "      BatchNorm2d-67           [-1, 40, 32, 32]              80\n",
      "      MBConvBlock-68           [-1, 40, 32, 32]               0\n",
      "         Identity-69           [-1, 40, 32, 32]               0\n",
      "Conv2dStaticSamePadding-70          [-1, 240, 32, 32]           9,600\n",
      "      BatchNorm2d-71          [-1, 240, 32, 32]             480\n",
      "MemoryEfficientSwish-72          [-1, 240, 32, 32]               0\n",
      "        ZeroPad2d-73          [-1, 240, 36, 36]               0\n",
      "Conv2dStaticSamePadding-74          [-1, 240, 32, 32]           6,000\n",
      "      BatchNorm2d-75          [-1, 240, 32, 32]             480\n",
      "MemoryEfficientSwish-76          [-1, 240, 32, 32]               0\n",
      "         Identity-77            [-1, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-78             [-1, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-79             [-1, 10, 1, 1]               0\n",
      "         Identity-80             [-1, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-81            [-1, 240, 1, 1]           2,640\n",
      "         Identity-82          [-1, 240, 32, 32]               0\n",
      "Conv2dStaticSamePadding-83           [-1, 40, 32, 32]           9,600\n",
      "      BatchNorm2d-84           [-1, 40, 32, 32]              80\n",
      "      MBConvBlock-85           [-1, 40, 32, 32]               0\n",
      "         Identity-86           [-1, 40, 32, 32]               0\n",
      "Conv2dStaticSamePadding-87          [-1, 240, 32, 32]           9,600\n",
      "      BatchNorm2d-88          [-1, 240, 32, 32]             480\n",
      "MemoryEfficientSwish-89          [-1, 240, 32, 32]               0\n",
      "        ZeroPad2d-90          [-1, 240, 33, 33]               0\n",
      "Conv2dStaticSamePadding-91          [-1, 240, 16, 16]           2,160\n",
      "      BatchNorm2d-92          [-1, 240, 16, 16]             480\n",
      "MemoryEfficientSwish-93          [-1, 240, 16, 16]               0\n",
      "         Identity-94            [-1, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-95             [-1, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-96             [-1, 10, 1, 1]               0\n",
      "         Identity-97             [-1, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-98            [-1, 240, 1, 1]           2,640\n",
      "         Identity-99          [-1, 240, 16, 16]               0\n",
      "Conv2dStaticSamePadding-100           [-1, 80, 16, 16]          19,200\n",
      "     BatchNorm2d-101           [-1, 80, 16, 16]             160\n",
      "     MBConvBlock-102           [-1, 80, 16, 16]               0\n",
      "        Identity-103           [-1, 80, 16, 16]               0\n",
      "Conv2dStaticSamePadding-104          [-1, 480, 16, 16]          38,400\n",
      "     BatchNorm2d-105          [-1, 480, 16, 16]             960\n",
      "MemoryEfficientSwish-106          [-1, 480, 16, 16]               0\n",
      "       ZeroPad2d-107          [-1, 480, 18, 18]               0\n",
      "Conv2dStaticSamePadding-108          [-1, 480, 16, 16]           4,320\n",
      "     BatchNorm2d-109          [-1, 480, 16, 16]             960\n",
      "MemoryEfficientSwish-110          [-1, 480, 16, 16]               0\n",
      "        Identity-111            [-1, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-112             [-1, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-113             [-1, 20, 1, 1]               0\n",
      "        Identity-114             [-1, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-115            [-1, 480, 1, 1]          10,080\n",
      "        Identity-116          [-1, 480, 16, 16]               0\n",
      "Conv2dStaticSamePadding-117           [-1, 80, 16, 16]          38,400\n",
      "     BatchNorm2d-118           [-1, 80, 16, 16]             160\n",
      "     MBConvBlock-119           [-1, 80, 16, 16]               0\n",
      "        Identity-120           [-1, 80, 16, 16]               0\n",
      "Conv2dStaticSamePadding-121          [-1, 480, 16, 16]          38,400\n",
      "     BatchNorm2d-122          [-1, 480, 16, 16]             960\n",
      "MemoryEfficientSwish-123          [-1, 480, 16, 16]               0\n",
      "       ZeroPad2d-124          [-1, 480, 18, 18]               0\n",
      "Conv2dStaticSamePadding-125          [-1, 480, 16, 16]           4,320\n",
      "     BatchNorm2d-126          [-1, 480, 16, 16]             960\n",
      "MemoryEfficientSwish-127          [-1, 480, 16, 16]               0\n",
      "        Identity-128            [-1, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-129             [-1, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-130             [-1, 20, 1, 1]               0\n",
      "        Identity-131             [-1, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-132            [-1, 480, 1, 1]          10,080\n",
      "        Identity-133          [-1, 480, 16, 16]               0\n",
      "Conv2dStaticSamePadding-134           [-1, 80, 16, 16]          38,400\n",
      "     BatchNorm2d-135           [-1, 80, 16, 16]             160\n",
      "     MBConvBlock-136           [-1, 80, 16, 16]               0\n",
      "        Identity-137           [-1, 80, 16, 16]               0\n",
      "Conv2dStaticSamePadding-138          [-1, 480, 16, 16]          38,400\n",
      "     BatchNorm2d-139          [-1, 480, 16, 16]             960\n",
      "MemoryEfficientSwish-140          [-1, 480, 16, 16]               0\n",
      "       ZeroPad2d-141          [-1, 480, 20, 20]               0\n",
      "Conv2dStaticSamePadding-142          [-1, 480, 16, 16]          12,000\n",
      "     BatchNorm2d-143          [-1, 480, 16, 16]             960\n",
      "MemoryEfficientSwish-144          [-1, 480, 16, 16]               0\n",
      "        Identity-145            [-1, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-146             [-1, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-147             [-1, 20, 1, 1]               0\n",
      "        Identity-148             [-1, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-149            [-1, 480, 1, 1]          10,080\n",
      "        Identity-150          [-1, 480, 16, 16]               0\n",
      "Conv2dStaticSamePadding-151          [-1, 112, 16, 16]          53,760\n",
      "     BatchNorm2d-152          [-1, 112, 16, 16]             224\n",
      "     MBConvBlock-153          [-1, 112, 16, 16]               0\n",
      "        Identity-154          [-1, 112, 16, 16]               0\n",
      "Conv2dStaticSamePadding-155          [-1, 672, 16, 16]          75,264\n",
      "     BatchNorm2d-156          [-1, 672, 16, 16]           1,344\n",
      "MemoryEfficientSwish-157          [-1, 672, 16, 16]               0\n",
      "       ZeroPad2d-158          [-1, 672, 20, 20]               0\n",
      "Conv2dStaticSamePadding-159          [-1, 672, 16, 16]          16,800\n",
      "     BatchNorm2d-160          [-1, 672, 16, 16]           1,344\n",
      "MemoryEfficientSwish-161          [-1, 672, 16, 16]               0\n",
      "        Identity-162            [-1, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-163             [-1, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-164             [-1, 28, 1, 1]               0\n",
      "        Identity-165             [-1, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-166            [-1, 672, 1, 1]          19,488\n",
      "        Identity-167          [-1, 672, 16, 16]               0\n",
      "Conv2dStaticSamePadding-168          [-1, 112, 16, 16]          75,264\n",
      "     BatchNorm2d-169          [-1, 112, 16, 16]             224\n",
      "     MBConvBlock-170          [-1, 112, 16, 16]               0\n",
      "        Identity-171          [-1, 112, 16, 16]               0\n",
      "Conv2dStaticSamePadding-172          [-1, 672, 16, 16]          75,264\n",
      "     BatchNorm2d-173          [-1, 672, 16, 16]           1,344\n",
      "MemoryEfficientSwish-174          [-1, 672, 16, 16]               0\n",
      "       ZeroPad2d-175          [-1, 672, 20, 20]               0\n",
      "Conv2dStaticSamePadding-176          [-1, 672, 16, 16]          16,800\n",
      "     BatchNorm2d-177          [-1, 672, 16, 16]           1,344\n",
      "MemoryEfficientSwish-178          [-1, 672, 16, 16]               0\n",
      "        Identity-179            [-1, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-180             [-1, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-181             [-1, 28, 1, 1]               0\n",
      "        Identity-182             [-1, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-183            [-1, 672, 1, 1]          19,488\n",
      "        Identity-184          [-1, 672, 16, 16]               0\n",
      "Conv2dStaticSamePadding-185          [-1, 112, 16, 16]          75,264\n",
      "     BatchNorm2d-186          [-1, 112, 16, 16]             224\n",
      "     MBConvBlock-187          [-1, 112, 16, 16]               0\n",
      "        Identity-188          [-1, 112, 16, 16]               0\n",
      "Conv2dStaticSamePadding-189          [-1, 672, 16, 16]          75,264\n",
      "     BatchNorm2d-190          [-1, 672, 16, 16]           1,344\n",
      "MemoryEfficientSwish-191          [-1, 672, 16, 16]               0\n",
      "       ZeroPad2d-192          [-1, 672, 19, 19]               0\n",
      "Conv2dStaticSamePadding-193            [-1, 672, 8, 8]          16,800\n",
      "     BatchNorm2d-194            [-1, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-195            [-1, 672, 8, 8]               0\n",
      "        Identity-196            [-1, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-197             [-1, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-198             [-1, 28, 1, 1]               0\n",
      "        Identity-199             [-1, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-200            [-1, 672, 1, 1]          19,488\n",
      "        Identity-201            [-1, 672, 8, 8]               0\n",
      "Conv2dStaticSamePadding-202            [-1, 192, 8, 8]         129,024\n",
      "     BatchNorm2d-203            [-1, 192, 8, 8]             384\n",
      "     MBConvBlock-204            [-1, 192, 8, 8]               0\n",
      "        Identity-205            [-1, 192, 8, 8]               0\n",
      "Conv2dStaticSamePadding-206           [-1, 1152, 8, 8]         221,184\n",
      "     BatchNorm2d-207           [-1, 1152, 8, 8]           2,304\n",
      "MemoryEfficientSwish-208           [-1, 1152, 8, 8]               0\n",
      "       ZeroPad2d-209         [-1, 1152, 12, 12]               0\n",
      "Conv2dStaticSamePadding-210           [-1, 1152, 8, 8]          28,800\n",
      "     BatchNorm2d-211           [-1, 1152, 8, 8]           2,304\n",
      "MemoryEfficientSwish-212           [-1, 1152, 8, 8]               0\n",
      "        Identity-213           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-214             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-215             [-1, 48, 1, 1]               0\n",
      "        Identity-216             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-217           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-218           [-1, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-219            [-1, 192, 8, 8]         221,184\n",
      "     BatchNorm2d-220            [-1, 192, 8, 8]             384\n",
      "     MBConvBlock-221            [-1, 192, 8, 8]               0\n",
      "        Identity-222            [-1, 192, 8, 8]               0\n",
      "Conv2dStaticSamePadding-223           [-1, 1152, 8, 8]         221,184\n",
      "     BatchNorm2d-224           [-1, 1152, 8, 8]           2,304\n",
      "MemoryEfficientSwish-225           [-1, 1152, 8, 8]               0\n",
      "       ZeroPad2d-226         [-1, 1152, 12, 12]               0\n",
      "Conv2dStaticSamePadding-227           [-1, 1152, 8, 8]          28,800\n",
      "     BatchNorm2d-228           [-1, 1152, 8, 8]           2,304\n",
      "MemoryEfficientSwish-229           [-1, 1152, 8, 8]               0\n",
      "        Identity-230           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-231             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-232             [-1, 48, 1, 1]               0\n",
      "        Identity-233             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-234           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-235           [-1, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-236            [-1, 192, 8, 8]         221,184\n",
      "     BatchNorm2d-237            [-1, 192, 8, 8]             384\n",
      "     MBConvBlock-238            [-1, 192, 8, 8]               0\n",
      "        Identity-239            [-1, 192, 8, 8]               0\n",
      "Conv2dStaticSamePadding-240           [-1, 1152, 8, 8]         221,184\n",
      "     BatchNorm2d-241           [-1, 1152, 8, 8]           2,304\n",
      "MemoryEfficientSwish-242           [-1, 1152, 8, 8]               0\n",
      "       ZeroPad2d-243         [-1, 1152, 12, 12]               0\n",
      "Conv2dStaticSamePadding-244           [-1, 1152, 8, 8]          28,800\n",
      "     BatchNorm2d-245           [-1, 1152, 8, 8]           2,304\n",
      "MemoryEfficientSwish-246           [-1, 1152, 8, 8]               0\n",
      "        Identity-247           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-248             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-249             [-1, 48, 1, 1]               0\n",
      "        Identity-250             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-251           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-252           [-1, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-253            [-1, 192, 8, 8]         221,184\n",
      "     BatchNorm2d-254            [-1, 192, 8, 8]             384\n",
      "     MBConvBlock-255            [-1, 192, 8, 8]               0\n",
      "        Identity-256            [-1, 192, 8, 8]               0\n",
      "Conv2dStaticSamePadding-257           [-1, 1152, 8, 8]         221,184\n",
      "     BatchNorm2d-258           [-1, 1152, 8, 8]           2,304\n",
      "MemoryEfficientSwish-259           [-1, 1152, 8, 8]               0\n",
      "       ZeroPad2d-260         [-1, 1152, 10, 10]               0\n",
      "Conv2dStaticSamePadding-261           [-1, 1152, 8, 8]          10,368\n",
      "     BatchNorm2d-262           [-1, 1152, 8, 8]           2,304\n",
      "MemoryEfficientSwish-263           [-1, 1152, 8, 8]               0\n",
      "        Identity-264           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-265             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-266             [-1, 48, 1, 1]               0\n",
      "        Identity-267             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-268           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-269           [-1, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-270            [-1, 320, 8, 8]         368,640\n",
      "     BatchNorm2d-271            [-1, 320, 8, 8]             640\n",
      "     MBConvBlock-272            [-1, 320, 8, 8]               0\n",
      "        Identity-273            [-1, 320, 8, 8]               0\n",
      "Conv2dStaticSamePadding-274           [-1, 1280, 8, 8]         409,600\n",
      "     BatchNorm2d-275           [-1, 1280, 8, 8]           2,560\n",
      "MemoryEfficientSwish-276           [-1, 1280, 8, 8]               0\n",
      "       ZeroPad2d-277          [-1, 1, 257, 257]               0\n",
      "Conv2dStaticSamePadding-278         [-1, 32, 128, 128]             288\n",
      "     BatchNorm2d-279         [-1, 32, 128, 128]              64\n",
      "MemoryEfficientSwish-280         [-1, 32, 128, 128]               0\n",
      "       ZeroPad2d-281         [-1, 32, 130, 130]               0\n",
      "Conv2dStaticSamePadding-282         [-1, 32, 128, 128]             288\n",
      "     BatchNorm2d-283         [-1, 32, 128, 128]              64\n",
      "MemoryEfficientSwish-284         [-1, 32, 128, 128]               0\n",
      "        Identity-285             [-1, 32, 1, 1]               0\n",
      "Conv2dStaticSamePadding-286              [-1, 8, 1, 1]             264\n",
      "MemoryEfficientSwish-287              [-1, 8, 1, 1]               0\n",
      "        Identity-288              [-1, 8, 1, 1]               0\n",
      "Conv2dStaticSamePadding-289             [-1, 32, 1, 1]             288\n",
      "        Identity-290         [-1, 32, 128, 128]               0\n",
      "Conv2dStaticSamePadding-291         [-1, 16, 128, 128]             512\n",
      "     BatchNorm2d-292         [-1, 16, 128, 128]              32\n",
      "     MBConvBlock-293         [-1, 16, 128, 128]               0\n",
      "        Identity-294         [-1, 16, 128, 128]               0\n",
      "Conv2dStaticSamePadding-295         [-1, 96, 128, 128]           1,536\n",
      "     BatchNorm2d-296         [-1, 96, 128, 128]             192\n",
      "MemoryEfficientSwish-297         [-1, 96, 128, 128]               0\n",
      "       ZeroPad2d-298         [-1, 96, 129, 129]               0\n",
      "Conv2dStaticSamePadding-299           [-1, 96, 64, 64]             864\n",
      "     BatchNorm2d-300           [-1, 96, 64, 64]             192\n",
      "MemoryEfficientSwish-301           [-1, 96, 64, 64]               0\n",
      "        Identity-302             [-1, 96, 1, 1]               0\n",
      "Conv2dStaticSamePadding-303              [-1, 4, 1, 1]             388\n",
      "MemoryEfficientSwish-304              [-1, 4, 1, 1]               0\n",
      "        Identity-305              [-1, 4, 1, 1]               0\n",
      "Conv2dStaticSamePadding-306             [-1, 96, 1, 1]             480\n",
      "        Identity-307           [-1, 96, 64, 64]               0\n",
      "Conv2dStaticSamePadding-308           [-1, 24, 64, 64]           2,304\n",
      "     BatchNorm2d-309           [-1, 24, 64, 64]              48\n",
      "     MBConvBlock-310           [-1, 24, 64, 64]               0\n",
      "        Identity-311           [-1, 24, 64, 64]               0\n",
      "Conv2dStaticSamePadding-312          [-1, 144, 64, 64]           3,456\n",
      "     BatchNorm2d-313          [-1, 144, 64, 64]             288\n",
      "MemoryEfficientSwish-314          [-1, 144, 64, 64]               0\n",
      "       ZeroPad2d-315          [-1, 144, 66, 66]               0\n",
      "Conv2dStaticSamePadding-316          [-1, 144, 64, 64]           1,296\n",
      "     BatchNorm2d-317          [-1, 144, 64, 64]             288\n",
      "MemoryEfficientSwish-318          [-1, 144, 64, 64]               0\n",
      "        Identity-319            [-1, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-320              [-1, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-321              [-1, 6, 1, 1]               0\n",
      "        Identity-322              [-1, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-323            [-1, 144, 1, 1]           1,008\n",
      "        Identity-324          [-1, 144, 64, 64]               0\n",
      "Conv2dStaticSamePadding-325           [-1, 24, 64, 64]           3,456\n",
      "     BatchNorm2d-326           [-1, 24, 64, 64]              48\n",
      "     MBConvBlock-327           [-1, 24, 64, 64]               0\n",
      "        Identity-328           [-1, 24, 64, 64]               0\n",
      "Conv2dStaticSamePadding-329          [-1, 144, 64, 64]           3,456\n",
      "     BatchNorm2d-330          [-1, 144, 64, 64]             288\n",
      "MemoryEfficientSwish-331          [-1, 144, 64, 64]               0\n",
      "       ZeroPad2d-332          [-1, 144, 67, 67]               0\n",
      "Conv2dStaticSamePadding-333          [-1, 144, 32, 32]           3,600\n",
      "     BatchNorm2d-334          [-1, 144, 32, 32]             288\n",
      "MemoryEfficientSwish-335          [-1, 144, 32, 32]               0\n",
      "        Identity-336            [-1, 144, 1, 1]               0\n",
      "Conv2dStaticSamePadding-337              [-1, 6, 1, 1]             870\n",
      "MemoryEfficientSwish-338              [-1, 6, 1, 1]               0\n",
      "        Identity-339              [-1, 6, 1, 1]               0\n",
      "Conv2dStaticSamePadding-340            [-1, 144, 1, 1]           1,008\n",
      "        Identity-341          [-1, 144, 32, 32]               0\n",
      "Conv2dStaticSamePadding-342           [-1, 40, 32, 32]           5,760\n",
      "     BatchNorm2d-343           [-1, 40, 32, 32]              80\n",
      "     MBConvBlock-344           [-1, 40, 32, 32]               0\n",
      "        Identity-345           [-1, 40, 32, 32]               0\n",
      "Conv2dStaticSamePadding-346          [-1, 240, 32, 32]           9,600\n",
      "     BatchNorm2d-347          [-1, 240, 32, 32]             480\n",
      "MemoryEfficientSwish-348          [-1, 240, 32, 32]               0\n",
      "       ZeroPad2d-349          [-1, 240, 36, 36]               0\n",
      "Conv2dStaticSamePadding-350          [-1, 240, 32, 32]           6,000\n",
      "     BatchNorm2d-351          [-1, 240, 32, 32]             480\n",
      "MemoryEfficientSwish-352          [-1, 240, 32, 32]               0\n",
      "        Identity-353            [-1, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-354             [-1, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-355             [-1, 10, 1, 1]               0\n",
      "        Identity-356             [-1, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-357            [-1, 240, 1, 1]           2,640\n",
      "        Identity-358          [-1, 240, 32, 32]               0\n",
      "Conv2dStaticSamePadding-359           [-1, 40, 32, 32]           9,600\n",
      "     BatchNorm2d-360           [-1, 40, 32, 32]              80\n",
      "     MBConvBlock-361           [-1, 40, 32, 32]               0\n",
      "        Identity-362           [-1, 40, 32, 32]               0\n",
      "Conv2dStaticSamePadding-363          [-1, 240, 32, 32]           9,600\n",
      "     BatchNorm2d-364          [-1, 240, 32, 32]             480\n",
      "MemoryEfficientSwish-365          [-1, 240, 32, 32]               0\n",
      "       ZeroPad2d-366          [-1, 240, 33, 33]               0\n",
      "Conv2dStaticSamePadding-367          [-1, 240, 16, 16]           2,160\n",
      "     BatchNorm2d-368          [-1, 240, 16, 16]             480\n",
      "MemoryEfficientSwish-369          [-1, 240, 16, 16]               0\n",
      "        Identity-370            [-1, 240, 1, 1]               0\n",
      "Conv2dStaticSamePadding-371             [-1, 10, 1, 1]           2,410\n",
      "MemoryEfficientSwish-372             [-1, 10, 1, 1]               0\n",
      "        Identity-373             [-1, 10, 1, 1]               0\n",
      "Conv2dStaticSamePadding-374            [-1, 240, 1, 1]           2,640\n",
      "        Identity-375          [-1, 240, 16, 16]               0\n",
      "Conv2dStaticSamePadding-376           [-1, 80, 16, 16]          19,200\n",
      "     BatchNorm2d-377           [-1, 80, 16, 16]             160\n",
      "     MBConvBlock-378           [-1, 80, 16, 16]               0\n",
      "        Identity-379           [-1, 80, 16, 16]               0\n",
      "Conv2dStaticSamePadding-380          [-1, 480, 16, 16]          38,400\n",
      "     BatchNorm2d-381          [-1, 480, 16, 16]             960\n",
      "MemoryEfficientSwish-382          [-1, 480, 16, 16]               0\n",
      "       ZeroPad2d-383          [-1, 480, 18, 18]               0\n",
      "Conv2dStaticSamePadding-384          [-1, 480, 16, 16]           4,320\n",
      "     BatchNorm2d-385          [-1, 480, 16, 16]             960\n",
      "MemoryEfficientSwish-386          [-1, 480, 16, 16]               0\n",
      "        Identity-387            [-1, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-388             [-1, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-389             [-1, 20, 1, 1]               0\n",
      "        Identity-390             [-1, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-391            [-1, 480, 1, 1]          10,080\n",
      "        Identity-392          [-1, 480, 16, 16]               0\n",
      "Conv2dStaticSamePadding-393           [-1, 80, 16, 16]          38,400\n",
      "     BatchNorm2d-394           [-1, 80, 16, 16]             160\n",
      "     MBConvBlock-395           [-1, 80, 16, 16]               0\n",
      "        Identity-396           [-1, 80, 16, 16]               0\n",
      "Conv2dStaticSamePadding-397          [-1, 480, 16, 16]          38,400\n",
      "     BatchNorm2d-398          [-1, 480, 16, 16]             960\n",
      "MemoryEfficientSwish-399          [-1, 480, 16, 16]               0\n",
      "       ZeroPad2d-400          [-1, 480, 18, 18]               0\n",
      "Conv2dStaticSamePadding-401          [-1, 480, 16, 16]           4,320\n",
      "     BatchNorm2d-402          [-1, 480, 16, 16]             960\n",
      "MemoryEfficientSwish-403          [-1, 480, 16, 16]               0\n",
      "        Identity-404            [-1, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-405             [-1, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-406             [-1, 20, 1, 1]               0\n",
      "        Identity-407             [-1, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-408            [-1, 480, 1, 1]          10,080\n",
      "        Identity-409          [-1, 480, 16, 16]               0\n",
      "Conv2dStaticSamePadding-410           [-1, 80, 16, 16]          38,400\n",
      "     BatchNorm2d-411           [-1, 80, 16, 16]             160\n",
      "     MBConvBlock-412           [-1, 80, 16, 16]               0\n",
      "        Identity-413           [-1, 80, 16, 16]               0\n",
      "Conv2dStaticSamePadding-414          [-1, 480, 16, 16]          38,400\n",
      "     BatchNorm2d-415          [-1, 480, 16, 16]             960\n",
      "MemoryEfficientSwish-416          [-1, 480, 16, 16]               0\n",
      "       ZeroPad2d-417          [-1, 480, 20, 20]               0\n",
      "Conv2dStaticSamePadding-418          [-1, 480, 16, 16]          12,000\n",
      "     BatchNorm2d-419          [-1, 480, 16, 16]             960\n",
      "MemoryEfficientSwish-420          [-1, 480, 16, 16]               0\n",
      "        Identity-421            [-1, 480, 1, 1]               0\n",
      "Conv2dStaticSamePadding-422             [-1, 20, 1, 1]           9,620\n",
      "MemoryEfficientSwish-423             [-1, 20, 1, 1]               0\n",
      "        Identity-424             [-1, 20, 1, 1]               0\n",
      "Conv2dStaticSamePadding-425            [-1, 480, 1, 1]          10,080\n",
      "        Identity-426          [-1, 480, 16, 16]               0\n",
      "Conv2dStaticSamePadding-427          [-1, 112, 16, 16]          53,760\n",
      "     BatchNorm2d-428          [-1, 112, 16, 16]             224\n",
      "     MBConvBlock-429          [-1, 112, 16, 16]               0\n",
      "        Identity-430          [-1, 112, 16, 16]               0\n",
      "Conv2dStaticSamePadding-431          [-1, 672, 16, 16]          75,264\n",
      "     BatchNorm2d-432          [-1, 672, 16, 16]           1,344\n",
      "MemoryEfficientSwish-433          [-1, 672, 16, 16]               0\n",
      "       ZeroPad2d-434          [-1, 672, 20, 20]               0\n",
      "Conv2dStaticSamePadding-435          [-1, 672, 16, 16]          16,800\n",
      "     BatchNorm2d-436          [-1, 672, 16, 16]           1,344\n",
      "MemoryEfficientSwish-437          [-1, 672, 16, 16]               0\n",
      "        Identity-438            [-1, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-439             [-1, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-440             [-1, 28, 1, 1]               0\n",
      "        Identity-441             [-1, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-442            [-1, 672, 1, 1]          19,488\n",
      "        Identity-443          [-1, 672, 16, 16]               0\n",
      "Conv2dStaticSamePadding-444          [-1, 112, 16, 16]          75,264\n",
      "     BatchNorm2d-445          [-1, 112, 16, 16]             224\n",
      "     MBConvBlock-446          [-1, 112, 16, 16]               0\n",
      "        Identity-447          [-1, 112, 16, 16]               0\n",
      "Conv2dStaticSamePadding-448          [-1, 672, 16, 16]          75,264\n",
      "     BatchNorm2d-449          [-1, 672, 16, 16]           1,344\n",
      "MemoryEfficientSwish-450          [-1, 672, 16, 16]               0\n",
      "       ZeroPad2d-451          [-1, 672, 20, 20]               0\n",
      "Conv2dStaticSamePadding-452          [-1, 672, 16, 16]          16,800\n",
      "     BatchNorm2d-453          [-1, 672, 16, 16]           1,344\n",
      "MemoryEfficientSwish-454          [-1, 672, 16, 16]               0\n",
      "        Identity-455            [-1, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-456             [-1, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-457             [-1, 28, 1, 1]               0\n",
      "        Identity-458             [-1, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-459            [-1, 672, 1, 1]          19,488\n",
      "        Identity-460          [-1, 672, 16, 16]               0\n",
      "Conv2dStaticSamePadding-461          [-1, 112, 16, 16]          75,264\n",
      "     BatchNorm2d-462          [-1, 112, 16, 16]             224\n",
      "     MBConvBlock-463          [-1, 112, 16, 16]               0\n",
      "        Identity-464          [-1, 112, 16, 16]               0\n",
      "Conv2dStaticSamePadding-465          [-1, 672, 16, 16]          75,264\n",
      "     BatchNorm2d-466          [-1, 672, 16, 16]           1,344\n",
      "MemoryEfficientSwish-467          [-1, 672, 16, 16]               0\n",
      "       ZeroPad2d-468          [-1, 672, 19, 19]               0\n",
      "Conv2dStaticSamePadding-469            [-1, 672, 8, 8]          16,800\n",
      "     BatchNorm2d-470            [-1, 672, 8, 8]           1,344\n",
      "MemoryEfficientSwish-471            [-1, 672, 8, 8]               0\n",
      "        Identity-472            [-1, 672, 1, 1]               0\n",
      "Conv2dStaticSamePadding-473             [-1, 28, 1, 1]          18,844\n",
      "MemoryEfficientSwish-474             [-1, 28, 1, 1]               0\n",
      "        Identity-475             [-1, 28, 1, 1]               0\n",
      "Conv2dStaticSamePadding-476            [-1, 672, 1, 1]          19,488\n",
      "        Identity-477            [-1, 672, 8, 8]               0\n",
      "Conv2dStaticSamePadding-478            [-1, 192, 8, 8]         129,024\n",
      "     BatchNorm2d-479            [-1, 192, 8, 8]             384\n",
      "     MBConvBlock-480            [-1, 192, 8, 8]               0\n",
      "        Identity-481            [-1, 192, 8, 8]               0\n",
      "Conv2dStaticSamePadding-482           [-1, 1152, 8, 8]         221,184\n",
      "     BatchNorm2d-483           [-1, 1152, 8, 8]           2,304\n",
      "MemoryEfficientSwish-484           [-1, 1152, 8, 8]               0\n",
      "       ZeroPad2d-485         [-1, 1152, 12, 12]               0\n",
      "Conv2dStaticSamePadding-486           [-1, 1152, 8, 8]          28,800\n",
      "     BatchNorm2d-487           [-1, 1152, 8, 8]           2,304\n",
      "MemoryEfficientSwish-488           [-1, 1152, 8, 8]               0\n",
      "        Identity-489           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-490             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-491             [-1, 48, 1, 1]               0\n",
      "        Identity-492             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-493           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-494           [-1, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-495            [-1, 192, 8, 8]         221,184\n",
      "     BatchNorm2d-496            [-1, 192, 8, 8]             384\n",
      "     MBConvBlock-497            [-1, 192, 8, 8]               0\n",
      "        Identity-498            [-1, 192, 8, 8]               0\n",
      "Conv2dStaticSamePadding-499           [-1, 1152, 8, 8]         221,184\n",
      "     BatchNorm2d-500           [-1, 1152, 8, 8]           2,304\n",
      "MemoryEfficientSwish-501           [-1, 1152, 8, 8]               0\n",
      "       ZeroPad2d-502         [-1, 1152, 12, 12]               0\n",
      "Conv2dStaticSamePadding-503           [-1, 1152, 8, 8]          28,800\n",
      "     BatchNorm2d-504           [-1, 1152, 8, 8]           2,304\n",
      "MemoryEfficientSwish-505           [-1, 1152, 8, 8]               0\n",
      "        Identity-506           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-507             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-508             [-1, 48, 1, 1]               0\n",
      "        Identity-509             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-510           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-511           [-1, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-512            [-1, 192, 8, 8]         221,184\n",
      "     BatchNorm2d-513            [-1, 192, 8, 8]             384\n",
      "     MBConvBlock-514            [-1, 192, 8, 8]               0\n",
      "        Identity-515            [-1, 192, 8, 8]               0\n",
      "Conv2dStaticSamePadding-516           [-1, 1152, 8, 8]         221,184\n",
      "     BatchNorm2d-517           [-1, 1152, 8, 8]           2,304\n",
      "MemoryEfficientSwish-518           [-1, 1152, 8, 8]               0\n",
      "       ZeroPad2d-519         [-1, 1152, 12, 12]               0\n",
      "Conv2dStaticSamePadding-520           [-1, 1152, 8, 8]          28,800\n",
      "     BatchNorm2d-521           [-1, 1152, 8, 8]           2,304\n",
      "MemoryEfficientSwish-522           [-1, 1152, 8, 8]               0\n",
      "        Identity-523           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-524             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-525             [-1, 48, 1, 1]               0\n",
      "        Identity-526             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-527           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-528           [-1, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-529            [-1, 192, 8, 8]         221,184\n",
      "     BatchNorm2d-530            [-1, 192, 8, 8]             384\n",
      "     MBConvBlock-531            [-1, 192, 8, 8]               0\n",
      "        Identity-532            [-1, 192, 8, 8]               0\n",
      "Conv2dStaticSamePadding-533           [-1, 1152, 8, 8]         221,184\n",
      "     BatchNorm2d-534           [-1, 1152, 8, 8]           2,304\n",
      "MemoryEfficientSwish-535           [-1, 1152, 8, 8]               0\n",
      "       ZeroPad2d-536         [-1, 1152, 10, 10]               0\n",
      "Conv2dStaticSamePadding-537           [-1, 1152, 8, 8]          10,368\n",
      "     BatchNorm2d-538           [-1, 1152, 8, 8]           2,304\n",
      "MemoryEfficientSwish-539           [-1, 1152, 8, 8]               0\n",
      "        Identity-540           [-1, 1152, 1, 1]               0\n",
      "Conv2dStaticSamePadding-541             [-1, 48, 1, 1]          55,344\n",
      "MemoryEfficientSwish-542             [-1, 48, 1, 1]               0\n",
      "        Identity-543             [-1, 48, 1, 1]               0\n",
      "Conv2dStaticSamePadding-544           [-1, 1152, 1, 1]          56,448\n",
      "        Identity-545           [-1, 1152, 8, 8]               0\n",
      "Conv2dStaticSamePadding-546            [-1, 320, 8, 8]         368,640\n",
      "     BatchNorm2d-547            [-1, 320, 8, 8]             640\n",
      "     MBConvBlock-548            [-1, 320, 8, 8]               0\n",
      "        Identity-549            [-1, 320, 8, 8]               0\n",
      "Conv2dStaticSamePadding-550           [-1, 1280, 8, 8]         409,600\n",
      "     BatchNorm2d-551           [-1, 1280, 8, 8]           2,560\n",
      "MemoryEfficientSwish-552           [-1, 1280, 8, 8]               0\n",
      "          Linear-553             [-1, 16, 1024]      10,486,784\n",
      "         Dropout-554             [-1, 17, 1024]               0\n",
      "       LayerNorm-555             [-1, 17, 1024]           2,048\n",
      "          Linear-556             [-1, 17, 1536]       1,572,864\n",
      "         Softmax-557            [-1, 8, 17, 17]               0\n",
      "          Linear-558             [-1, 17, 1024]         525,312\n",
      "         Dropout-559             [-1, 17, 1024]               0\n",
      "       Attention-560             [-1, 17, 1024]               0\n",
      "         PreNorm-561             [-1, 17, 1024]               0\n",
      "       LayerNorm-562             [-1, 17, 1024]           2,048\n",
      "          Linear-563             [-1, 17, 2048]       2,099,200\n",
      "            GELU-564             [-1, 17, 2048]               0\n",
      "         Dropout-565             [-1, 17, 2048]               0\n",
      "          Linear-566             [-1, 17, 1024]       2,098,176\n",
      "         Dropout-567             [-1, 17, 1024]               0\n",
      "     FeedForward-568             [-1, 17, 1024]               0\n",
      "         PreNorm-569             [-1, 17, 1024]               0\n",
      "       LayerNorm-570             [-1, 17, 1024]           2,048\n",
      "          Linear-571             [-1, 17, 1536]       1,572,864\n",
      "         Softmax-572            [-1, 8, 17, 17]               0\n",
      "          Linear-573             [-1, 17, 1024]         525,312\n",
      "         Dropout-574             [-1, 17, 1024]               0\n",
      "       Attention-575             [-1, 17, 1024]               0\n",
      "         PreNorm-576             [-1, 17, 1024]               0\n",
      "       LayerNorm-577             [-1, 17, 1024]           2,048\n",
      "          Linear-578             [-1, 17, 2048]       2,099,200\n",
      "            GELU-579             [-1, 17, 2048]               0\n",
      "         Dropout-580             [-1, 17, 2048]               0\n",
      "          Linear-581             [-1, 17, 1024]       2,098,176\n",
      "         Dropout-582             [-1, 17, 1024]               0\n",
      "     FeedForward-583             [-1, 17, 1024]               0\n",
      "         PreNorm-584             [-1, 17, 1024]               0\n",
      "       LayerNorm-585             [-1, 17, 1024]           2,048\n",
      "          Linear-586             [-1, 17, 1536]       1,572,864\n",
      "         Softmax-587            [-1, 8, 17, 17]               0\n",
      "          Linear-588             [-1, 17, 1024]         525,312\n",
      "         Dropout-589             [-1, 17, 1024]               0\n",
      "       Attention-590             [-1, 17, 1024]               0\n",
      "         PreNorm-591             [-1, 17, 1024]               0\n",
      "       LayerNorm-592             [-1, 17, 1024]           2,048\n",
      "          Linear-593             [-1, 17, 2048]       2,099,200\n",
      "            GELU-594             [-1, 17, 2048]               0\n",
      "         Dropout-595             [-1, 17, 2048]               0\n",
      "          Linear-596             [-1, 17, 1024]       2,098,176\n",
      "         Dropout-597             [-1, 17, 1024]               0\n",
      "     FeedForward-598             [-1, 17, 1024]               0\n",
      "         PreNorm-599             [-1, 17, 1024]               0\n",
      "       LayerNorm-600             [-1, 17, 1024]           2,048\n",
      "          Linear-601             [-1, 17, 1536]       1,572,864\n",
      "         Softmax-602            [-1, 8, 17, 17]               0\n",
      "          Linear-603             [-1, 17, 1024]         525,312\n",
      "         Dropout-604             [-1, 17, 1024]               0\n",
      "       Attention-605             [-1, 17, 1024]               0\n",
      "         PreNorm-606             [-1, 17, 1024]               0\n",
      "       LayerNorm-607             [-1, 17, 1024]           2,048\n",
      "          Linear-608             [-1, 17, 2048]       2,099,200\n",
      "            GELU-609             [-1, 17, 2048]               0\n",
      "         Dropout-610             [-1, 17, 2048]               0\n",
      "          Linear-611             [-1, 17, 1024]       2,098,176\n",
      "         Dropout-612             [-1, 17, 1024]               0\n",
      "     FeedForward-613             [-1, 17, 1024]               0\n",
      "         PreNorm-614             [-1, 17, 1024]               0\n",
      "       LayerNorm-615             [-1, 17, 1024]           2,048\n",
      "          Linear-616             [-1, 17, 1536]       1,572,864\n",
      "         Softmax-617            [-1, 8, 17, 17]               0\n",
      "          Linear-618             [-1, 17, 1024]         525,312\n",
      "         Dropout-619             [-1, 17, 1024]               0\n",
      "       Attention-620             [-1, 17, 1024]               0\n",
      "         PreNorm-621             [-1, 17, 1024]               0\n",
      "       LayerNorm-622             [-1, 17, 1024]           2,048\n",
      "          Linear-623             [-1, 17, 2048]       2,099,200\n",
      "            GELU-624             [-1, 17, 2048]               0\n",
      "         Dropout-625             [-1, 17, 2048]               0\n",
      "          Linear-626             [-1, 17, 1024]       2,098,176\n",
      "         Dropout-627             [-1, 17, 1024]               0\n",
      "     FeedForward-628             [-1, 17, 1024]               0\n",
      "         PreNorm-629             [-1, 17, 1024]               0\n",
      "       LayerNorm-630             [-1, 17, 1024]           2,048\n",
      "          Linear-631             [-1, 17, 1536]       1,572,864\n",
      "         Softmax-632            [-1, 8, 17, 17]               0\n",
      "          Linear-633             [-1, 17, 1024]         525,312\n",
      "         Dropout-634             [-1, 17, 1024]               0\n",
      "       Attention-635             [-1, 17, 1024]               0\n",
      "         PreNorm-636             [-1, 17, 1024]               0\n",
      "       LayerNorm-637             [-1, 17, 1024]           2,048\n",
      "          Linear-638             [-1, 17, 2048]       2,099,200\n",
      "            GELU-639             [-1, 17, 2048]               0\n",
      "         Dropout-640             [-1, 17, 2048]               0\n",
      "          Linear-641             [-1, 17, 1024]       2,098,176\n",
      "         Dropout-642             [-1, 17, 1024]               0\n",
      "     FeedForward-643             [-1, 17, 1024]               0\n",
      "         PreNorm-644             [-1, 17, 1024]               0\n",
      "     Transformer-645             [-1, 17, 1024]               0\n",
      "        Identity-646                 [-1, 1024]               0\n",
      "          Linear-647                 [-1, 2048]       2,099,200\n",
      "            ReLU-648                 [-1, 2048]               0\n",
      "          Linear-649                    [-1, 1]           2,049\n",
      "         Sigmoid-650                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 58,400,441\n",
      "Trainable params: 58,400,441\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 49152.00\n",
      "Forward/backward pass size (MB): 566.89\n",
      "Params size (MB): 222.78\n",
      "Estimated Total Size (MB): 49941.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from einops import rearrange\n",
    "\n",
    "import sys\n",
    "from model.backbone.efficient_net.model import EfficientNet\n",
    "\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re, math\n",
    "from model.vision_transformer.vit import Transformer\n",
    "\n",
    "\n",
    "class DualEfficientViT(nn.Module):\n",
    "    def __init__(self, channels=1280,\\\n",
    "                 image_size=224,patch_size=7,num_classes=1,dim=1024,\\\n",
    "                 depth=6,heads=8,mlp_dim=2048,\\\n",
    "                 emb_dim=32, dim_head=64,dropout=0.15,emb_dropout=0.15,version=\"cross_attention-spatial-cat\",weight=0.5):  \n",
    "        super(DualEfficientViT, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dim_head = dim_head\n",
    "        self.dropout_value = dropout\n",
    "        self.emb_dropout = emb_dropout\n",
    "        self.features_size = {\n",
    "            128: (4, 4),\n",
    "            224: (7, 7),\n",
    "            256: (8, 8)\n",
    "        }\n",
    "        \n",
    "        # \"cross_attention-spatial-cat\": sá»­ dá»¥ng cross-attention, cat vá»i spatial vectors output\n",
    "        # \"cross_attention-spatial-add\": sá»­ dá»¥ng cross-attention, add vá»i spatial vectors output\n",
    "        # \"cross_attention-freq-cat\": sá»­ dá»¥ng cross-attention, cat vá»i freq vectors\n",
    "        # \"cross_attention-freq-add\": sá»­ dá»¥ng cross-attention, add vá»i freq vectors\n",
    "        # \"merge-add\": cá»ng tháº³ng 2 vectors spatial vÃ  freq, cÃ³ weight: spatial + weight*freq\n",
    "        # \"merge-cat\": cat tháº³ng 2 vectors spatial vÃ  freq, cÃ³ weight: spatial + weight*freq\n",
    "        self.version = version\n",
    "        self.weight = weight\n",
    "\n",
    "        self.spatial_extractor = self.get_feature_extractor(num_classes=num_classes, in_channels=3)   # efficient_net-b0, return shape (1280, 8, 8) or (1280, 7, 7)\n",
    "        self.freq_extractor = self.get_feature_extractor(num_classes=num_classes, in_channels=1)\n",
    "\n",
    "        ############################# XÃ©t 2 stream hiá»n táº¡i lÃ  nhÆ° nhau\n",
    "        # KÃ­ch thÆ°á»c cá»§a 1 patch\n",
    "        self.patch_size = patch_size\n",
    "    \n",
    "        # Sá» lÆ°á»£ng patches\n",
    "        self.num_patches = int((self.features_size[image_size][0] * self.features_size[image_size][1]) / (self.patch_size * self.patch_size))\n",
    "        # Patch_dim = P^2 * C\n",
    "        self.patch_dim = channels * (self.patch_size ** 2)\n",
    "\n",
    "        # print(\"Num patches: \", self.num_patches)\n",
    "        # print(\"Patch dim: \", self.patch_dim)\n",
    "\n",
    "        # Embed vá» trÃ­ cho tá»«ng patch\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches+1, self.dim))\n",
    "        # self.pos_embedding_1 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "        # self.pos_embedding_2 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "        # self.pos_embedding_3 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "\n",
    "        # ÄÆ°a flatten vector cá»§a feature maps vá» chiá»u cá» Äá»nh cá»§a vector trong transformer.\n",
    "        # self.patch_to_embedding_1 = nn.Linear(self.patch_dim, self.dim)\n",
    "        # self.patch_to_embedding_2 = nn.Linear(self.patch_dim, self.dim)\n",
    "\n",
    "        # Giáº£m chiá»u vector sau concat 2*patch_dim vá» D:\n",
    "        self.patch_to_embedding_cat = nn.Linear(2*self.patch_dim, self.dim)\n",
    "        self.patch_to_embedding_add = nn.Linear(self.patch_dim, self.dim)\n",
    "\n",
    "        # ThÃªm 1 embedding vector cho classify token:\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(self.emb_dropout)\n",
    "        self.transformer = Transformer(self.dim, self.depth, self.heads, self.dim_head, self.mlp_dim, self.dropout_value)\n",
    "\n",
    "        self.to_cls_token = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(self.dim, self.mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.mlp_dim, self.num_classes)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def get_feature_extractor(self, architecture=\"efficient_net\", pretrained=\"\", num_classes=1, in_channels=3):\n",
    "        extractor = None\n",
    "        if architecture == \"efficient_net\":\n",
    "            if pretrained == \"\":\n",
    "                extractor = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes,in_channels = in_channels)\n",
    "                print(\"Extractor: \", extractor)\n",
    "            else:\n",
    "                extractor = EfficientNet.from_pretrained('efficientnet-b7', num_classes=num_classes,in_channels = in_channels)\n",
    "                # Load checkpoint\n",
    "                checkpoint = torch.load(pretrained, map_location=\"cpu\")\n",
    "                state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
    "                # Load weights\n",
    "                extractor.load_state_dict({re.sub(\"^module.\", \"\", k): v for k, v in state_dict.items()}, strict=False)\n",
    "\n",
    "            # Freeze the first (num_blocks - 3) blocks and unfreeze the rest \n",
    "            # for i in range(0, len(extractor._blocks)):\n",
    "            #     for index, param in enumerate(extractor._blocks[i].parameters()):\n",
    "            #         if i >= len(extractor._blocks) - 3:\n",
    "            #             param.requires_grad = True\n",
    "            #         else:\n",
    "            #             param.requires_grad = False\n",
    "        return extractor\n",
    "\n",
    "    \"\"\"\n",
    "        Get from torch.nn.MultiheadAttention\n",
    "        scale-dot: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L4966\n",
    "        multi-head: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L5059\n",
    "    \"\"\"\n",
    "    def scale_dot(self, q, k, v, attn_mask=None, dropout_p=0):\n",
    "        B, Nt, E = q.shape\n",
    "        q = q / math.sqrt(E)\n",
    "        # (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\n",
    "        attn = torch.bmm(q, k.transpose(-2, -1))\n",
    "        if attn_mask is not None:\n",
    "            attn += attn_mask\n",
    "        attn = torch.nn.functional.softmax(attn, dim=-1)\n",
    "        if dropout_p > 0.0:\n",
    "            attn = torch.nn.functional.dropout(attn, p=dropout_p)\n",
    "        # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\n",
    "        output = torch.bmm(attn, v)\n",
    "        return output, attn\n",
    "\n",
    "    def cross_attention(self, spatials, ifreqs):\n",
    "        \"\"\"\n",
    "            spatials: (B, N, D) --> Query,\n",
    "            freqs: (B, N, D) --> Key\n",
    "            output: \n",
    "        \"\"\"\n",
    "        emb_dim = spatials.shape[2]\n",
    "        assert emb_dim == ifreqs.shape[2]\n",
    "        attn_outputs, attn_weights = self.scale_dot(spatials, ifreqs, ifreqs)\n",
    "        return attn_outputs, attn_weights\n",
    "\n",
    "\n",
    "    def forward(self, spatial_imgs, frequency_imgs):\n",
    "        p = self.patch_size\n",
    "        # Extract features\n",
    "        spatial_features = self.spatial_extractor.extract_features(spatial_imgs)                 # shape (batchsize, 1280, 8, 8)\n",
    "        freq_features = self.freq_extractor.extract_features(frequency_imgs)                     # shape (batchsize, 1280, 8, 8)conda\n",
    "        ifreq_features = torch.log(torch.abs(torch.fft.ifft2(torch.fft.ifftshift(freq_features))) + 1e-10)  # HÆ¡i áº£o???\n",
    "        # print(ifreq_features.shape)\n",
    "        # assert(ifreq_features.shape == freq_features.shape)\n",
    "        # print(\"Features shape: \", spatial_features.shape, freq_features.shape)\n",
    "\n",
    "        # Flatten to vector:\n",
    "        spatial_vectors = rearrange(spatial_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        freq_vectors = rearrange(freq_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        ifreq_vectors = rearrange(ifreq_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "\n",
    "        assert self.patch_dim == spatial_vectors.shape[2]\n",
    "        assert self.patch_dim == freq_vectors.shape[2]\n",
    "\n",
    "        embed = None\n",
    "        \n",
    "        if \"cross_attention\" in self.version:          # Merge using cross-attention  \n",
    "            ########## Patch embedding and add position embedding to each domain:\n",
    "            # spatial_vectors = self.patch_to_embedding_1(spatial_vectors)\n",
    "            # spatial_vectors += self.pos_embedding_1\n",
    "\n",
    "            # freq_vectors = self.patch_to_embedding_2(freq_vectors)\n",
    "            # freq_vectors += self.pos_embedding_2\n",
    "\n",
    "            # ifreq_vectors = self.patch_to_embedding_2(ifreq_vectors)\n",
    "            # ifreq_vectors += self.pos_embedding_2  \n",
    "            # print(\"Step 2 shape: \", spatial_vectors.shape, freq_vectors.shape)  # (batchsize, num_patches, D)\n",
    "            ##########\n",
    "        \n",
    "            # Cal attn weight between ifreq and spatial vectors:\n",
    "            # Cross-attention (spatial-decoder, ifreq-encoder)\n",
    "            attn_outputs, attn_weights = self.cross_attention(spatial_vectors, ifreq_vectors)     # Shape: (), (batchsize, num_patches, num_patches)\n",
    "            if \"freq\" in self.version:          # Get attention in frequency domain:\n",
    "                out_attn = torch.bmm(attn_weights, freq_vectors)\n",
    "            elif \"spatial\" in self.version:     # Get attention in spatial domain:\n",
    "                out_attn = torch.bmm(attn_weights, ifreq_vectors)\n",
    "                ### Check correct bmm:\n",
    "                # print(torch.eq(attn_outputs, out_attn))\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # Concat or add and linear\n",
    "            # print(\"Spatial vectors: \", spatial_vectors.shape)\n",
    "            # print(spatial_vectors)\n",
    "            # print(\"Output attention: \", out_attn.shape)\n",
    "            # print(out_attn)\n",
    "            if \"add\" in self.version:\n",
    "                out = torch.add(spatial_vectors, self.weight * out_attn)\n",
    "                # print(\"Out\", out)\n",
    "                embed = self.patch_to_embedding_add(out)                 # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            elif \"cat\" in self.version:\n",
    "                out = torch.cat([spatial_vectors, self.weight * out_attn], dim=2)\n",
    "                embed = self.patch_to_embedding_cat(out)                 # Shape: (batchsize, num_patches, 2*patch_dim) => (batchsize, num_patches, dim)\n",
    "            else:\n",
    "                pass\n",
    "        else:   # Merge directly\n",
    "            if \"add\" in self.version:\n",
    "                out = torch.add(spatial_vectors, self.weight * freq_vectors)\n",
    "                embed = self.patch_to_embedding_add(out)                # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            elif \"cat\" in self.version:\n",
    "                out = torch.cat([spatial_vectors, self.weight * freq_vectors], dim=2)\n",
    "                embed = self.patch_to_embedding_cat(out)                # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        # print(\"Embeded shape: \", embed.shape)\n",
    "\n",
    "        # Expand classify token to batchsize and add to patch embeddings:\n",
    "        cls_tokens = self.cls_token.expand(embed.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, embed), dim=1)   # (batchsize, num_patches+1, dim)\n",
    "        x += self.pos_embedding\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        x = self.mlp_head(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x = torch.ones(32, 3, 256, 256)\n",
    "    y = torch.ones(32, 1, 256, 256)\n",
    "    model_ = DualEfficientViT(image_size=256, patch_size=2)\n",
    "    out = model_(x, y)\n",
    "    print(out.shape)\n",
    "    from torchsummary import summary\n",
    "    summary(model_, [(3, 256, 256), (1, 256, 256)], device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from einops import rearrange\n",
    "\n",
    "import sys\n",
    "from model.backbone.efficient_net.model import EfficientNet\n",
    "\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re, math\n",
    "from model.vision_transformer.vit import Transformer\n",
    "\n",
    "\n",
    "class DualEfficientViTV2(nn.Module):\n",
    "    def __init__(self, channels=1280,\\\n",
    "                 image_size=224,patch_size=7,num_classes=1,dim=1024,\\\n",
    "                 depth=6,heads=8,mlp_dim=2048,\\\n",
    "                 emb_dim=32, dim_head=64,dropout=0.15,emb_dropout=0.15,version=\"cross_attention-spatial-cat\",weight=0.5):  \n",
    "        super(DualEfficientViTV2, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dim_head = dim_head\n",
    "        self.dropout_value = dropout\n",
    "        self.emb_dropout = emb_dropout\n",
    "        self.features_size = {\n",
    "            128: (4, 4),\n",
    "            224: (7, 7),\n",
    "            256: (8, 8)\n",
    "        }\n",
    "        \n",
    "        # \"cross_attention-spatial-cat\": sá»­ dá»¥ng cross-attention, cat vá»i spatial vectors output\n",
    "        # \"cross_attention-spatial-add\": sá»­ dá»¥ng cross-attention, add vá»i spatial vectors output\n",
    "        # \"cross_attention-freq-cat\": sá»­ dá»¥ng cross-attention, cat vá»i freq vectors\n",
    "        # \"cross_attention-freq-add\": sá»­ dá»¥ng cross-attention, add vá»i freq vectors\n",
    "        # \"merge-add\": cá»ng tháº³ng 2 vectors spatial vÃ  freq, cÃ³ weight: spatial + weight*freq\n",
    "        # \"merge-cat\": cat tháº³ng 2 vectors spatial vÃ  freq, cÃ³ weight: spatial + weight*freq\n",
    "        self.version = version\n",
    "        self.weight = weight\n",
    "\n",
    "        self.spatial_extractor = self.get_feature_extractor(num_classes=num_classes, in_channels=3)   # efficient_net-b0, return shape (1280, 8, 8) or (1280, 7, 7)\n",
    "        self.freq_extractor = self.get_feature_extractor(num_classes=num_classes, in_channels=1)\n",
    "\n",
    "        ############################# XÃ©t 2 stream hiá»n táº¡i lÃ  nhÆ° nhau\n",
    "        # KÃ­ch thÆ°á»c cá»§a 1 patch\n",
    "        self.patch_size = patch_size\n",
    "    \n",
    "        # Sá» lÆ°á»£ng patches\n",
    "        self.num_patches = int((self.features_size[image_size][0] * self.features_size[image_size][1]) / (self.patch_size * self.patch_size))\n",
    "        # Patch_dim = P^2 * C\n",
    "        self.patch_dim = channels * (self.patch_size ** 2)\n",
    "\n",
    "        # print(\"Num patches: \", self.num_patches)\n",
    "        # print(\"Patch dim: \", self.patch_dim)\n",
    "\n",
    "        # Embed vá» trÃ­ cho tá»«ng patch\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches+1, self.dim))\n",
    "        # self.pos_embedding_1 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "        # self.pos_embedding_2 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "        # self.pos_embedding_3 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "\n",
    "        # ÄÆ°a flatten vector cá»§a feature maps vá» chiá»u cá» Äá»nh cá»§a vector trong transformer.\n",
    "        # self.patch_to_embedding_1 = nn.Linear(self.patch_dim, self.dim)\n",
    "        # self.patch_to_embedding_2 = nn.Linear(self.patch_dim, self.dim)\n",
    "\n",
    "        # Giáº£m chiá»u vector sau concat 2*patch_dim vá» D:\n",
    "        self.patch_to_embedding_cat = nn.Linear(2*self.patch_dim, self.dim)\n",
    "        self.patch_to_embedding_add = nn.Linear(self.patch_dim, self.dim)\n",
    "\n",
    "        # ThÃªm 1 embedding vector cho classify token:\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(self.emb_dropout)\n",
    "        self.transformer = Transformer(self.dim, self.depth, self.heads, self.dim_head, self.mlp_dim, self.dropout_value)\n",
    "\n",
    "        self.to_cls_token = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(self.dim, self.mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.mlp_dim, self.num_classes)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def get_feature_extractor(self, architecture=\"efficient_net\", pretrained=\"\", num_classes=1, in_channels=3):\n",
    "        extractor = None\n",
    "        if architecture == \"efficient_net\":\n",
    "            if pretrained == \"\":\n",
    "                extractor = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes,in_channels = in_channels)\n",
    "            else:\n",
    "                extractor = EfficientNet.from_pretrained('efficientnet-b7', num_classes=num_classes,in_channels = in_channels)\n",
    "                # Load checkpoint\n",
    "                checkpoint = torch.load(pretrained, map_location=\"cpu\")\n",
    "                state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
    "                # Load weights\n",
    "                extractor.load_state_dict({re.sub(\"^module.\", \"\", k): v for k, v in state_dict.items()}, strict=False)\n",
    "\n",
    "            # Freeze the first (num_blocks - 3) blocks and unfreeze the rest \n",
    "            # for i in range(0, len(extractor._blocks)):\n",
    "            #     for index, param in enumerate(extractor._blocks[i].parameters()):\n",
    "            #         if i >= len(extractor._blocks) - 3:\n",
    "            #             param.requires_grad = True\n",
    "            #         else:\n",
    "            #             param.requires_grad = False\n",
    "        return extractor\n",
    "\n",
    "    \"\"\"\n",
    "        Get from torch.nn.MultiheadAttention\n",
    "        scale-dot: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L4966\n",
    "        multi-head: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L5059\n",
    "    \"\"\"\n",
    "    def scale_dot(self, q, k, v, attn_mask=None, dropout_p=0):\n",
    "        B, Nt, E = q.shape\n",
    "        q = q / math.sqrt(E)\n",
    "        # (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\n",
    "        attn = torch.bmm(q, k.transpose(-2, -1))\n",
    "        if attn_mask is not None:\n",
    "            attn += attn_mask\n",
    "        attn = torch.nn.functional.softmax(attn, dim=-1)\n",
    "        if dropout_p > 0.0:\n",
    "            attn = torch.nn.functional.dropout(attn, p=dropout_p)\n",
    "        # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\n",
    "        output = torch.bmm(attn, v)\n",
    "        return output, attn\n",
    "\n",
    "    def cross_attention(self, spatials, freqs):\n",
    "        \"\"\"\n",
    "            spatials: (B, N, D) --> Query,\n",
    "            freqs: (B, N, D) --> Key\n",
    "            output: \n",
    "        \"\"\"\n",
    "        emb_dim = spatials.shape[2]\n",
    "        assert emb_dim == freqs.shape[2]\n",
    "        attn_outputs, attn_weights = self.scale_dot(spatials, freqs, freqs)\n",
    "        return attn_outputs, attn_weights\n",
    "\n",
    "\n",
    "    def forward(self, spatial_imgs, frequency_imgs):\n",
    "        p = self.patch_size\n",
    "        # Extract features\n",
    "        spatial_features = self.spatial_extractor.extract_features(spatial_imgs)                 # shape (batchsize, 1280, 8, 8)\n",
    "        freq_features = self.freq_extractor.extract_features(frequency_imgs)                     # shape (batchsize, 1280, 8, 8)\n",
    "        # print(\"Features shape: \", spatial_features.shape, freq_features.shape)\n",
    "\n",
    "        # Flatten to vector:\n",
    "        spatial_vectors = rearrange(spatial_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        freq_vectors = rearrange(freq_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "\n",
    "        assert self.patch_dim == spatial_vectors.shape[2]\n",
    "        assert self.patch_dim == freq_vectors.shape[2]\n",
    "\n",
    "        embed = None\n",
    "        \n",
    "        if \"cross_attention\" in self.version:          # Merge using cross-attention  \n",
    "            ########## Patch embedding and add position embedding to each domain:\n",
    "            attn_outputs, attn_weights = self.cross_attention(spatial_vectors, freq_vectors)     # Shape: (), (batchsize, num_patches, num_patches)\n",
    "            out_attn = torch.bmm(attn_weights, freq_vectors)\n",
    "\n",
    "            # Concat or add and linear\n",
    "            # print(\"Spatial vectors: \", spatial_vectors.shape)\n",
    "            # print(spatial_vectors)\n",
    "            # print(\"Output attention: \", out_attn.shape)\n",
    "            # print(out_attn)\n",
    "            if \"add\" in self.version:\n",
    "                out = torch.add(spatial_vectors, self.weight * out_attn)\n",
    "                # print(\"Out\", out)\n",
    "                embed = self.patch_to_embedding_add(out)                 # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            elif \"cat\" in self.version:\n",
    "                out = torch.cat([spatial_vectors, self.weight * out_attn], dim=2)\n",
    "                embed = self.patch_to_embedding_cat(out)                 # Shape: (batchsize, num_patches, 2*patch_dim) => (batchsize, num_patches, dim)\n",
    "            else:\n",
    "                pass\n",
    "        else:   # Merge directly\n",
    "            if \"add\" in self.version:\n",
    "                out = torch.add(spatial_vectors, self.weight * freq_vectors)\n",
    "                embed = self.patch_to_embedding_add(out)                # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            elif \"cat\" in self.version:\n",
    "                out = torch.cat([spatial_vectors, self.weight * freq_vectors], dim=2)\n",
    "                embed = self.patch_to_embedding_cat(out)                # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        # print(\"Embeded shape: \", embed.shape)\n",
    "\n",
    "        # Expand classify token to batchsize and add to patch embeddings:\n",
    "        cls_tokens = self.cls_token.expand(embed.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, embed), dim=1)   # (batchsize, num_patches+1, dim)\n",
    "        x += self.pos_embedding\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        x = self.mlp_head(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x = torch.ones(32, 3, 256, 256)\n",
    "    y = torch.ones(32, 1, 256, 256)\n",
    "    model_ = DualEfficientViTV2(image_size=256, patch_size=2)\n",
    "    out = model_(x, y)\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from einops import rearrange\n",
    "\n",
    "import sys\n",
    "from model.backbone.efficient_net.model import EfficientNet\n",
    "\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re, math\n",
    "from model.vision_transformer.vit import Transformer\n",
    "\n",
    "\n",
    "class DualCrossAttnEfficient(nn.Module):\n",
    "    def __init__(self, channels=1280,\\\n",
    "                 image_size=224,patch_size=7,num_classes=1,dim=1024,\\\n",
    "                 depth=6,heads=8,mlp_dim=2048,\\\n",
    "                 emb_dim=32, dim_head=64,dropout=0.15,emb_dropout=0.15,version=\"cross_attention-spatial-cat\",weight=0.5,freeze=0):  \n",
    "        super(DualCrossAttnEfficient, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dim_head = dim_head\n",
    "        self.dropout_value = dropout\n",
    "        self.emb_dropout = emb_dropout\n",
    "        self.features_size = {\n",
    "            128: (4, 4),\n",
    "            224: (7, 7),\n",
    "            256: (8, 8)\n",
    "        }\n",
    "        \n",
    "        # \"cross_attention-spatial-cat\": sá»­ dá»¥ng cross-attention, cat vá»i spatial vectors output\n",
    "        # \"cross_attention-spatial-add\": sá»­ dá»¥ng cross-attention, add vá»i spatial vectors output\n",
    "        # \"cross_attention-freq-cat\": sá»­ dá»¥ng cross-attention, cat vá»i freq vectors\n",
    "        # \"cross_attention-freq-add\": sá»­ dá»¥ng cross-attention, add vá»i freq vectors\n",
    "        # \"merge-add\": cá»ng tháº³ng 2 vectors spatial vÃ  freq, cÃ³ weight: spatial + weight*freq\n",
    "        # \"merge-cat\": cat tháº³ng 2 vectors spatial vÃ  freq, cÃ³ weight: spatial + weight*freq\n",
    "        self.version = version\n",
    "        self.weight = weight\n",
    "\n",
    "        self.spatial_extractor = self.get_feature_extractor(freeze=freeze, num_classes=num_classes, in_channels=3)   # efficient_net-b0, return shape (1280, 8, 8) or (1280, 7, 7)\n",
    "        self.freq_extractor = self.get_feature_extractor(freeze=freeze, num_classes=num_classes, in_channels=1)\n",
    "\n",
    "        ############################# XÃ©t 2 stream hiá»n táº¡i lÃ  nhÆ° nhau\n",
    "        # KÃ­ch thÆ°á»c cá»§a 1 patch\n",
    "        self.patch_size = patch_size\n",
    "    \n",
    "        # Sá» lÆ°á»£ng patches\n",
    "        self.num_patches = int((self.features_size[image_size][0] * self.features_size[image_size][1]) / (self.patch_size * self.patch_size))\n",
    "        # Patch_dim = P^2 * C\n",
    "        self.patch_dim = channels * (self.patch_size ** 2)\n",
    "\n",
    "        # print(\"Num patches: \", self.num_patches)\n",
    "        # print(\"Patch dim: \", self.patch_dim)\n",
    "\n",
    "        # Embed vá» trÃ­ cho tá»«ng patch\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches+1, self.dim))\n",
    "        # self.pos_embedding_1 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "        # self.pos_embedding_2 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "        # self.pos_embedding_3 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "\n",
    "        # ÄÆ°a flatten vector cá»§a feature maps vá» chiá»u cá» Äá»nh cá»§a vector trong transformer.\n",
    "        # self.patch_to_embedding_1 = nn.Linear(self.patch_dim, self.dim)\n",
    "        # self.patch_to_embedding_2 = nn.Linear(self.patch_dim, self.dim)\n",
    "\n",
    "        # Giáº£m chiá»u vector sau concat 2*patch_dim vá» D:\n",
    "        self.patch_to_embedding_cat = nn.Linear(2*self.patch_dim, self.dim)\n",
    "        self.patch_to_embedding_add = nn.Linear(self.patch_dim, self.dim)\n",
    "\n",
    "        # ThÃªm 1 embedding vector cho classify token:\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(self.emb_dropout)\n",
    "        self.transformer = Transformer(self.dim, self.depth, self.heads, self.dim_head, self.mlp_dim, self.dropout_value)\n",
    "\n",
    "        self.to_cls_token = nn.Identity()\n",
    "        \n",
    "        self.reduce_dim = nn.Linear(self.num_patches, 1)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(self.dim, self.mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.mlp_dim, self.num_classes)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def get_feature_extractor(self, architecture=\"efficient_net\", freeze=0, pretrained=\"\", num_classes=1, in_channels=3):\n",
    "        extractor = None\n",
    "        if architecture == \"efficient_net\":\n",
    "            if pretrained == \"\":\n",
    "                extractor = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes,in_channels = in_channels)\n",
    "            else:\n",
    "                extractor = EfficientNet.from_pretrained('efficientnet-b7', num_classes=num_classes,in_channels = in_channels)\n",
    "                # Load checkpoint\n",
    "                checkpoint = torch.load(pretrained, map_location=\"cpu\")\n",
    "                state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
    "                # Load weights\n",
    "                extractor.load_state_dict({re.sub(\"^module.\", \"\", k): v for k, v in state_dict.items()}, strict=False)\n",
    "\n",
    "            if freeze:\n",
    "            # Freeze the first (num_blocks - 3) blocks and unfreeze the rest \n",
    "                for i in range(0, len(extractor._blocks)):\n",
    "                    for index, param in enumerate(extractor._blocks[i].parameters()):\n",
    "                        if i >= len(extractor._blocks) - 3:\n",
    "                            param.requires_grad = True\n",
    "                        else:\n",
    "                            param.requires_grad = False\n",
    "        return extractor\n",
    "\n",
    "    \"\"\"\n",
    "        Get from torch.nn.MultiheadAttention\n",
    "        scale-dot: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L4966\n",
    "        multi-head: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L5059\n",
    "    \"\"\"\n",
    "    def scale_dot(self, q, k, v, attn_mask=None, dropout_p=0):\n",
    "        B, Nt, E = q.shape\n",
    "        q = q / math.sqrt(E)\n",
    "        # (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\n",
    "        attn = torch.bmm(q, k.transpose(-2, -1))\n",
    "        if attn_mask is not None:\n",
    "            attn += attn_mask\n",
    "        attn = torch.nn.functional.softmax(attn, dim=-1)\n",
    "        if dropout_p > 0.0:\n",
    "            attn = torch.nn.functional.dropout(attn, p=dropout_p)\n",
    "        # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\n",
    "        output = torch.bmm(attn, v)\n",
    "        return output, attn\n",
    "\n",
    "    def cross_attention(self, spatials, ifreqs):\n",
    "        \"\"\"\n",
    "            spatials: (B, N, D) --> Query,\n",
    "            freqs: (B, N, D) --> Key\n",
    "            output: \n",
    "        \"\"\"\n",
    "        emb_dim = spatials.shape[2]\n",
    "        assert emb_dim == ifreqs.shape[2]\n",
    "        attn_outputs, attn_weights = self.scale_dot(spatials, ifreqs, ifreqs)\n",
    "        return attn_outputs, attn_weights\n",
    "\n",
    "\n",
    "    def forward(self, spatial_imgs, frequency_imgs):\n",
    "        p = self.patch_size\n",
    "        # Extract features\n",
    "        spatial_features = self.spatial_extractor.extract_features(spatial_imgs)                 # shape (batchsize, 1280, 8, 8)\n",
    "        freq_features = self.freq_extractor.extract_features(frequency_imgs)                     # shape (batchsize, 1280, 8, 8)conda\n",
    "        ifreq_features = torch.log(torch.abs(torch.fft.ifft2(torch.fft.ifftshift(freq_features))) + 1e-10)  # HÆ¡i áº£o???\n",
    "        # print(ifreq_features.shape)\n",
    "        # assert(ifreq_features.shape == freq_features.shape)\n",
    "        # print(\"Features shape: \", spatial_features.shape, freq_features.shape)\n",
    "\n",
    "        # Flatten to vector:\n",
    "        spatial_vectors = rearrange(spatial_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        freq_vectors = rearrange(freq_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        ifreq_vectors = rearrange(ifreq_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "\n",
    "        assert self.patch_dim == spatial_vectors.shape[2]\n",
    "        assert self.patch_dim == freq_vectors.shape[2]\n",
    "\n",
    "        embed = None\n",
    "        \n",
    "        if \"cross_attention\" in self.version:          # Merge using cross-attention  \n",
    "            ########## Patch embedding and add position embedding to each domain:\n",
    "            # spatial_vectors = self.patch_to_embedding_1(spatial_vectors)\n",
    "            # spatial_vectors += self.pos_embedding_1\n",
    "\n",
    "            # freq_vectors = self.patch_to_embedding_2(freq_vectors)\n",
    "            # freq_vectors += self.pos_embedding_2\n",
    "\n",
    "            # ifreq_vectors = self.patch_to_embedding_2(ifreq_vectors)\n",
    "            # ifreq_vectors += self.pos_embedding_2  \n",
    "            # print(\"Step 2 shape: \", spatial_vectors.shape, freq_vectors.shape)  # (batchsize, num_patches, D)\n",
    "            ##########\n",
    "        \n",
    "            # Cal attn weight between ifreq and spatial vectors:\n",
    "            # Cross-attention (spatial-decoder, ifreq-encoder)\n",
    "            attn_outputs, attn_weights = self.cross_attention(spatial_vectors, ifreq_vectors)     # Shape: (), (batchsize, num_patches, num_patches)\n",
    "            if \"freq\" in self.version:          # Get attention in frequency domain:\n",
    "                out_attn = torch.bmm(attn_weights, freq_vectors)\n",
    "            elif \"spatial\" in self.version:     # Get attention in spatial domain:\n",
    "                out_attn = torch.bmm(attn_weights, ifreq_vectors)\n",
    "                ### Check correct bmm:\n",
    "                # print(torch.eq(attn_outputs, out_attn))\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # Concat or add and linear\n",
    "            # print(\"Spatial vectors: \", spatial_vectors.shape)\n",
    "            # print(spatial_vectors)\n",
    "            # print(\"Output attention: \", out_attn.shape)\n",
    "            # print(out_attn)\n",
    "            if \"add\" in self.version:\n",
    "                out = torch.add(spatial_vectors, self.weight * out_attn)\n",
    "                # print(\"Out\", out)\n",
    "                embed = self.patch_to_embedding_add(out)                 # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            elif \"cat\" in self.version:\n",
    "                out = torch.cat([spatial_vectors, self.weight * out_attn], dim=2)\n",
    "                embed = self.patch_to_embedding_cat(out)                 # Shape: (batchsize, num_patches, 2*patch_dim) => (batchsize, num_patches, dim)\n",
    "            else:\n",
    "                pass\n",
    "        else:   # Merge directly\n",
    "            if \"add\" in self.version:\n",
    "                out = torch.add(spatial_vectors, self.weight * freq_vectors)\n",
    "                embed = self.patch_to_embedding_add(out)                # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            elif \"cat\" in self.version:\n",
    "                out = torch.cat([spatial_vectors, self.weight * freq_vectors], dim=2)\n",
    "                embed = self.patch_to_embedding_cat(out)                # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        # print(\"Embeded shape: \", embed.shape)\n",
    "\n",
    "        # Expand classify token to batchsize and add to patch embeddings:\n",
    "        print(\"Embed:\", embed.shape)\n",
    "        embed = embed.permute(0, 2, 1)\n",
    "        x = self.reduce_dim(embed)\n",
    "        print(\"Before squeeze:\", x.shape)\n",
    "        x = x.squeeze(-1)\n",
    "        print(\"Before mlp:\", x.shape)\n",
    "        x = self.mlp_head(x)\n",
    "        print(\"After mlp: \", x.shape)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x = torch.ones(32, 3, 256, 256)\n",
    "    y = torch.ones(32, 1, 256, 256)\n",
    "    model_ = DualCrossAttnEfficient(image_size=256, patch_size=2)\n",
    "    device = torch.device(\"cpu\")\n",
    "    import os\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "    model_.to(device)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    out = model_(x, y)\n",
    "    print(out.shape)\n",
    "    \n",
    "    from torchsummary import summary\n",
    "    summary(model_, [(3, 256, 256), (1, 256, 256)], device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "2 2\n",
      "3 3\n",
      "4 4\n"
     ]
    }
   ],
   "source": [
    "for a, b in zip(['1', '2', '3', '4'], [1, 2, 3, 4]):\n",
    "    print(a, b)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c99a311309def2227702d5f2c551c7a298c8ef3054a27c10387f667008a8451"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('phucnp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
