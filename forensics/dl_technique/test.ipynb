{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiotlab/anaconda3/envs/phucnp/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3666],\n",
       "        [0.3717],\n",
       "        [0.3337]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.vision_transformer.dual_efficient_vit import DualEfficientViT\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "\n",
    "torch.manual_seed(0)\n",
    "image = torch.randn((3, 3, 128, 128))\n",
    "freq = torch.randn((3, 1, 128, 128))\n",
    "\n",
    "model = DualEfficientViT(image_size=128, patch_size=1, version=\"cross_attention-freq-add\", weight=0.5)\n",
    "\n",
    "out = model(image, freq)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST\n",
      "target =  tensor([1., 0., 0., 0., 1., 0.])\n",
      "pred =  tensor([0.3251, 0.0902, 0.3936, 0.6069, 0.1743, 0.4743])\n",
      "tensor([1.1237, 0.0945, 0.5003, 0.9336, 1.7472, 0.6431])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8404)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class WeightedBinaryCrossEntropy(nn.Module):\n",
    "    def __init__(self, weights=None, eps=1e-7):\n",
    "        super(WeightedBinaryCrossEntropy, self).__init__()\n",
    "        self.weights = weights      # [class_0, class_1]\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, pred, target):    # target: (batch_size, ), pred: (batch_size, )\n",
    "        pred = pred.clamp(self.eps, 1-self.eps)\n",
    "        if self.weights is not None:\n",
    "            loss = self.weights[1] * (target * torch.log(pred)) + self.weights[0] * ((1- target) * torch.log(1-pred))\n",
    "        else:\n",
    "            loss = target * torch.log(pred) + (1 - target) * torch.log(1-pred)\n",
    "        return torch.mean(torch.neg(loss))\n",
    "    \n",
    "# class_0, class_1 = 1, 4\n",
    "# total = class_0 + class_1\n",
    "# weights = [0.5 * total/class_0, 0.5 * total/class_1]\n",
    "# wbce = WeightedBinaryCrossEntropy(weights=weights)\n",
    "\n",
    "# print(weights)\n",
    "# target = torch.tensor([0.0, 1.0, 1.0, 1.0, 1.0], dtype=torch.float32)\n",
    "# pred = torch.tensor([0.4, 0.6, 1.0, 1.0, 0.6])\n",
    "\n",
    "# loss = wbce(target, pred)\n",
    "# print(loss)\n",
    "\n",
    "print(\"TEST\")\n",
    "bce = nn.BCELoss(reduction='none')\n",
    "wbce = WeightedBinaryCrossEntropy(weights=[1,1])\n",
    "pred = torch.rand((6))\n",
    "tar = torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float32)\n",
    "print(\"target = \", tar )\n",
    "print(\"pred = \", pred)\n",
    "print(bce(pred, tar))\n",
    "wbce(pred, tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weights=None, gamma=0, eps=1e-7):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.weights = weights      # [class_0, class_1] và class_0 + class_1 = 1\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.clamp(self.eps, 1 - self.eps)\n",
    "        if self.weights is not None:\n",
    "            loss = self.weights[1] * ((1 - pred) ** self.gamma) * target * torch.log(pred) + self.weights[0] * (pred ** self.gamma) * (1 - target) * torch.log(1 - pred)\n",
    "        else:\n",
    "            loss = ((1 - pred) * self.gamma) * target * torch.log(pred) + (pred ** self.gamma) * (1 - target) * torch.log(1 - pred)\n",
    "        return torch.mean(torch.neg(loss))\n",
    "\n",
    "print(\"Test loss\")\n",
    "torch.manual_seed(4)\n",
    "# target = torch.tensor([0.0, 1.0, 1.0, 1.0, 1.0], dtype=torch.float32)\n",
    "# pred = torch.tensor([0.2, 0.8, 0.8, 0.8, 0.8])\n",
    "\n",
    "batch = 4\n",
    "pred = torch.rand((batch, ))\n",
    "tar = torch.randint(0, 2, (batch, ))\n",
    "num_0 = (tar > 0).to(torch.int32).sum().item()\n",
    "num_1 = (tar == 0).to(torch.int32).sum(0).item()\n",
    "\n",
    "print(\"Pred: \", pred)\n",
    "print(\"Target: \", tar.shape)\n",
    "print(\"Numner: \", num_0, num_1)\n",
    "weights = [num_1 / (num_0 + num_1), num_0 / (num_0 + num_1)]\n",
    "print(weights)\n",
    "\n",
    "fl = FocalLoss(weights=weights, gamma=0.5)\n",
    "fcloss = fl(pred, tar)\n",
    "\n",
    "wbce = WeightedBinaryCrossEntropy(weights=weights)\n",
    "wbceloss = wbce(pred, tar)\n",
    "\n",
    "print(\"Focal loss: \", fcloss)\n",
    "print(\"WBCE loss: \",wbceloss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from einops import rearrange\n",
    "\n",
    "import sys\n",
    "from model.backbone.efficient_net.model import EfficientNet\n",
    "\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re, math\n",
    "from model.vision_transformer.vit import Transformer\n",
    "\n",
    "\n",
    "class DualEfficientViT(nn.Module):\n",
    "    def __init__(self, channels=1280,\\\n",
    "                 image_size=224,patch_size=7,num_classes=1,dim=1024,\\\n",
    "                 depth=6,heads=8,mlp_dim=2048,\\\n",
    "                 emb_dim=32, dim_head=64,dropout=0.15,emb_dropout=0.15,version=\"cross_attention-spatial-cat\",weight=0.5):  \n",
    "        super(DualEfficientViT, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dim_head = dim_head\n",
    "        self.dropout_value = dropout\n",
    "        self.emb_dropout = emb_dropout\n",
    "        self.features_size = {\n",
    "            128: (4, 4),\n",
    "            224: (7, 7),\n",
    "            256: (8, 8)\n",
    "        }\n",
    "        \n",
    "        # \"cross_attention-spatial-cat\": sử dụng cross-attention, cat với spatial vectors output\n",
    "        # \"cross_attention-spatial-add\": sử dụng cross-attention, add với spatial vectors output\n",
    "        # \"cross_attention-freq-cat\": sử dụng cross-attention, cat với freq vectors\n",
    "        # \"cross_attention-freq-add\": sử dụng cross-attention, add với freq vectors\n",
    "        # \"merge-add\": cộng thẳng 2 vectors spatial và freq, có weight: spatial + weight*freq\n",
    "        # \"merge-cat\": cat thẳng 2 vectors spatial và freq, có weight: spatial + weight*freq\n",
    "        self.version = version\n",
    "        self.weight = weight\n",
    "\n",
    "        self.spatial_extractor = self.get_feature_extractor(num_classes=num_classes, in_channels=3)   # efficient_net-b0, return shape (1280, 8, 8) or (1280, 7, 7)\n",
    "        self.freq_extractor = self.get_feature_extractor(num_classes=num_classes, in_channels=1)\n",
    "\n",
    "        ############################# Xét 2 stream hiện tại là như nhau\n",
    "        # Kích thước của 1 patch\n",
    "        self.patch_size = patch_size\n",
    "    \n",
    "        # Số lượng patches\n",
    "        self.num_patches = int((self.features_size[image_size][0] * self.features_size[image_size][1]) / (self.patch_size * self.patch_size))\n",
    "        # Patch_dim = P^2 * C\n",
    "        self.patch_dim = channels * (self.patch_size ** 2)\n",
    "\n",
    "        # print(\"Num patches: \", self.num_patches)\n",
    "        # print(\"Patch dim: \", self.patch_dim)\n",
    "\n",
    "        # Embed vị trí cho từng patch\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches+1, self.dim))\n",
    "        # self.pos_embedding_1 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "        # self.pos_embedding_2 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "        # self.pos_embedding_3 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "\n",
    "        # Đưa flatten vector của feature maps về chiều cố định của vector trong transformer.\n",
    "        # self.patch_to_embedding_1 = nn.Linear(self.patch_dim, self.dim)\n",
    "        # self.patch_to_embedding_2 = nn.Linear(self.patch_dim, self.dim)\n",
    "\n",
    "        # Giảm chiều vector sau concat 2*patch_dim về D:\n",
    "        self.patch_to_embedding_cat = nn.Linear(2*self.patch_dim, self.dim)\n",
    "        self.patch_to_embedding_add = nn.Linear(self.patch_dim, self.dim)\n",
    "\n",
    "        # Thêm 1 embedding vector cho classify token:\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(self.emb_dropout)\n",
    "        self.transformer = Transformer(self.dim, self.depth, self.heads, self.dim_head, self.mlp_dim, self.dropout_value)\n",
    "\n",
    "        self.to_cls_token = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(self.dim, self.mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.mlp_dim, self.num_classes)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def get_feature_extractor(self, architecture=\"efficient_net\", pretrained=\"\", num_classes=1, in_channels=3):\n",
    "        extractor = None\n",
    "        if architecture == \"efficient_net\":\n",
    "            if pretrained == \"\":\n",
    "                extractor = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes,in_channels = in_channels)\n",
    "                print(\"Extractor: \", extractor)\n",
    "            else:\n",
    "                extractor = EfficientNet.from_pretrained('efficientnet-b7', num_classes=num_classes,in_channels = in_channels)\n",
    "                # Load checkpoint\n",
    "                checkpoint = torch.load(pretrained, map_location=\"cpu\")\n",
    "                state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
    "                # Load weights\n",
    "                extractor.load_state_dict({re.sub(\"^module.\", \"\", k): v for k, v in state_dict.items()}, strict=False)\n",
    "\n",
    "            # Freeze the first (num_blocks - 3) blocks and unfreeze the rest \n",
    "            # for i in range(0, len(extractor._blocks)):\n",
    "            #     for index, param in enumerate(extractor._blocks[i].parameters()):\n",
    "            #         if i >= len(extractor._blocks) - 3:\n",
    "            #             param.requires_grad = True\n",
    "            #         else:\n",
    "            #             param.requires_grad = False\n",
    "        return extractor\n",
    "\n",
    "    \"\"\"\n",
    "        Get from torch.nn.MultiheadAttention\n",
    "        scale-dot: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L4966\n",
    "        multi-head: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L5059\n",
    "    \"\"\"\n",
    "    def scale_dot(self, q, k, v, attn_mask=None, dropout_p=0):\n",
    "        B, Nt, E = q.shape\n",
    "        q = q / math.sqrt(E)\n",
    "        # (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\n",
    "        attn = torch.bmm(q, k.transpose(-2, -1))\n",
    "        if attn_mask is not None:\n",
    "            attn += attn_mask\n",
    "        attn = torch.nn.functional.softmax(attn, dim=-1)\n",
    "        if dropout_p > 0.0:\n",
    "            attn = torch.nn.functional.dropout(attn, p=dropout_p)\n",
    "        # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\n",
    "        output = torch.bmm(attn, v)\n",
    "        return output, attn\n",
    "\n",
    "    def cross_attention(self, spatials, ifreqs):\n",
    "        \"\"\"\n",
    "            spatials: (B, N, D) --> Query,\n",
    "            freqs: (B, N, D) --> Key\n",
    "            output: \n",
    "        \"\"\"\n",
    "        emb_dim = spatials.shape[2]\n",
    "        assert emb_dim == ifreqs.shape[2]\n",
    "        attn_outputs, attn_weights = self.scale_dot(spatials, ifreqs, ifreqs)\n",
    "        return attn_outputs, attn_weights\n",
    "\n",
    "\n",
    "    def forward(self, spatial_imgs, frequency_imgs):\n",
    "        p = self.patch_size\n",
    "        # Extract features\n",
    "        spatial_features = self.spatial_extractor.extract_features(spatial_imgs)                 # shape (batchsize, 1280, 8, 8)\n",
    "        freq_features = self.freq_extractor.extract_features(frequency_imgs)                     # shape (batchsize, 1280, 8, 8)conda\n",
    "        ifreq_features = torch.log(torch.abs(torch.fft.ifft2(torch.fft.ifftshift(freq_features))) + 1e-10)  # Hơi ảo???\n",
    "        # print(ifreq_features.shape)\n",
    "        # assert(ifreq_features.shape == freq_features.shape)\n",
    "        # print(\"Features shape: \", spatial_features.shape, freq_features.shape)\n",
    "\n",
    "        # Flatten to vector:\n",
    "        spatial_vectors = rearrange(spatial_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        freq_vectors = rearrange(freq_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        ifreq_vectors = rearrange(ifreq_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "\n",
    "        assert self.patch_dim == spatial_vectors.shape[2]\n",
    "        assert self.patch_dim == freq_vectors.shape[2]\n",
    "\n",
    "        embed = None\n",
    "        \n",
    "        if \"cross_attention\" in self.version:          # Merge using cross-attention  \n",
    "            ########## Patch embedding and add position embedding to each domain:\n",
    "            # spatial_vectors = self.patch_to_embedding_1(spatial_vectors)\n",
    "            # spatial_vectors += self.pos_embedding_1\n",
    "\n",
    "            # freq_vectors = self.patch_to_embedding_2(freq_vectors)\n",
    "            # freq_vectors += self.pos_embedding_2\n",
    "\n",
    "            # ifreq_vectors = self.patch_to_embedding_2(ifreq_vectors)\n",
    "            # ifreq_vectors += self.pos_embedding_2  \n",
    "            # print(\"Step 2 shape: \", spatial_vectors.shape, freq_vectors.shape)  # (batchsize, num_patches, D)\n",
    "            ##########\n",
    "        \n",
    "            # Cal attn weight between ifreq and spatial vectors:\n",
    "            # Cross-attention (spatial-decoder, ifreq-encoder)\n",
    "            attn_outputs, attn_weights = self.cross_attention(spatial_vectors, ifreq_vectors)     # Shape: (), (batchsize, num_patches, num_patches)\n",
    "            if \"freq\" in self.version:          # Get attention in frequency domain:\n",
    "                out_attn = torch.bmm(attn_weights, freq_vectors)\n",
    "            elif \"spatial\" in self.version:     # Get attention in spatial domain:\n",
    "                out_attn = torch.bmm(attn_weights, ifreq_vectors)\n",
    "                ### Check correct bmm:\n",
    "                # print(torch.eq(attn_outputs, out_attn))\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # Concat or add and linear\n",
    "            # print(\"Spatial vectors: \", spatial_vectors.shape)\n",
    "            # print(spatial_vectors)\n",
    "            # print(\"Output attention: \", out_attn.shape)\n",
    "            # print(out_attn)\n",
    "            if \"add\" in self.version:\n",
    "                out = torch.add(spatial_vectors, self.weight * out_attn)\n",
    "                # print(\"Out\", out)\n",
    "                embed = self.patch_to_embedding_add(out)                 # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            elif \"cat\" in self.version:\n",
    "                out = torch.cat([spatial_vectors, self.weight * out_attn], dim=2)\n",
    "                embed = self.patch_to_embedding_cat(out)                 # Shape: (batchsize, num_patches, 2*patch_dim) => (batchsize, num_patches, dim)\n",
    "            else:\n",
    "                pass\n",
    "        else:   # Merge directly\n",
    "            if \"add\" in self.version:\n",
    "                out = torch.add(spatial_vectors, self.weight * freq_vectors)\n",
    "                embed = self.patch_to_embedding_add(out)                # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            elif \"cat\" in self.version:\n",
    "                out = torch.cat([spatial_vectors, self.weight * freq_vectors], dim=2)\n",
    "                embed = self.patch_to_embedding_cat(out)                # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        # print(\"Embeded shape: \", embed.shape)\n",
    "\n",
    "        # Expand classify token to batchsize and add to patch embeddings:\n",
    "        cls_tokens = self.cls_token.expand(embed.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, embed), dim=1)   # (batchsize, num_patches+1, dim)\n",
    "        x += self.pos_embedding\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        x = self.mlp_head(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x = torch.ones(32, 3, 256, 256)\n",
    "    y = torch.ones(32, 1, 256, 256)\n",
    "    model_ = DualEfficientViT(image_size=256, patch_size=2)\n",
    "    out = model_(x, y)\n",
    "    print(out.shape)\n",
    "    from torchsummary import summary\n",
    "    summary(model_, [(3, 256, 256), (1, 256, 256)], device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from einops import rearrange\n",
    "\n",
    "import sys\n",
    "from model.backbone.efficient_net.model import EfficientNet\n",
    "\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re, math\n",
    "from model.vision_transformer.vit import Transformer\n",
    "\n",
    "\n",
    "class DualEfficientViTV2(nn.Module):\n",
    "    def __init__(self, channels=1280,\\\n",
    "                 image_size=224,patch_size=7,num_classes=1,dim=1024,\\\n",
    "                 depth=6,heads=8,mlp_dim=2048,\\\n",
    "                 emb_dim=32, dim_head=64,dropout=0.15,emb_dropout=0.15,version=\"cross_attention-spatial-cat\",weight=0.5):  \n",
    "        super(DualEfficientViTV2, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dim_head = dim_head\n",
    "        self.dropout_value = dropout\n",
    "        self.emb_dropout = emb_dropout\n",
    "        self.features_size = {\n",
    "            128: (4, 4),\n",
    "            224: (7, 7),\n",
    "            256: (8, 8)\n",
    "        }\n",
    "        \n",
    "        # \"cross_attention-spatial-cat\": sử dụng cross-attention, cat với spatial vectors output\n",
    "        # \"cross_attention-spatial-add\": sử dụng cross-attention, add với spatial vectors output\n",
    "        # \"cross_attention-freq-cat\": sử dụng cross-attention, cat với freq vectors\n",
    "        # \"cross_attention-freq-add\": sử dụng cross-attention, add với freq vectors\n",
    "        # \"merge-add\": cộng thẳng 2 vectors spatial và freq, có weight: spatial + weight*freq\n",
    "        # \"merge-cat\": cat thẳng 2 vectors spatial và freq, có weight: spatial + weight*freq\n",
    "        self.version = version\n",
    "        self.weight = weight\n",
    "\n",
    "        self.spatial_extractor = self.get_feature_extractor(num_classes=num_classes, in_channels=3)   # efficient_net-b0, return shape (1280, 8, 8) or (1280, 7, 7)\n",
    "        self.freq_extractor = self.get_feature_extractor(num_classes=num_classes, in_channels=1)\n",
    "\n",
    "        ############################# Xét 2 stream hiện tại là như nhau\n",
    "        # Kích thước của 1 patch\n",
    "        self.patch_size = patch_size\n",
    "    \n",
    "        # Số lượng patches\n",
    "        self.num_patches = int((self.features_size[image_size][0] * self.features_size[image_size][1]) / (self.patch_size * self.patch_size))\n",
    "        # Patch_dim = P^2 * C\n",
    "        self.patch_dim = channels * (self.patch_size ** 2)\n",
    "\n",
    "        # print(\"Num patches: \", self.num_patches)\n",
    "        # print(\"Patch dim: \", self.patch_dim)\n",
    "\n",
    "        # Embed vị trí cho từng patch\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches+1, self.dim))\n",
    "        # self.pos_embedding_1 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "        # self.pos_embedding_2 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "        # self.pos_embedding_3 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "\n",
    "        # Đưa flatten vector của feature maps về chiều cố định của vector trong transformer.\n",
    "        # self.patch_to_embedding_1 = nn.Linear(self.patch_dim, self.dim)\n",
    "        # self.patch_to_embedding_2 = nn.Linear(self.patch_dim, self.dim)\n",
    "\n",
    "        # Giảm chiều vector sau concat 2*patch_dim về D:\n",
    "        self.patch_to_embedding_cat = nn.Linear(2*self.patch_dim, self.dim)\n",
    "        self.patch_to_embedding_add = nn.Linear(self.patch_dim, self.dim)\n",
    "\n",
    "        # Thêm 1 embedding vector cho classify token:\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(self.emb_dropout)\n",
    "        self.transformer = Transformer(self.dim, self.depth, self.heads, self.dim_head, self.mlp_dim, self.dropout_value)\n",
    "\n",
    "        self.to_cls_token = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(self.dim, self.mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.mlp_dim, self.num_classes)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def get_feature_extractor(self, architecture=\"efficient_net\", pretrained=\"\", num_classes=1, in_channels=3):\n",
    "        extractor = None\n",
    "        if architecture == \"efficient_net\":\n",
    "            if pretrained == \"\":\n",
    "                extractor = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes,in_channels = in_channels)\n",
    "            else:\n",
    "                extractor = EfficientNet.from_pretrained('efficientnet-b7', num_classes=num_classes,in_channels = in_channels)\n",
    "                # Load checkpoint\n",
    "                checkpoint = torch.load(pretrained, map_location=\"cpu\")\n",
    "                state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
    "                # Load weights\n",
    "                extractor.load_state_dict({re.sub(\"^module.\", \"\", k): v for k, v in state_dict.items()}, strict=False)\n",
    "\n",
    "            # Freeze the first (num_blocks - 3) blocks and unfreeze the rest \n",
    "            # for i in range(0, len(extractor._blocks)):\n",
    "            #     for index, param in enumerate(extractor._blocks[i].parameters()):\n",
    "            #         if i >= len(extractor._blocks) - 3:\n",
    "            #             param.requires_grad = True\n",
    "            #         else:\n",
    "            #             param.requires_grad = False\n",
    "        return extractor\n",
    "\n",
    "    \"\"\"\n",
    "        Get from torch.nn.MultiheadAttention\n",
    "        scale-dot: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L4966\n",
    "        multi-head: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L5059\n",
    "    \"\"\"\n",
    "    def scale_dot(self, q, k, v, attn_mask=None, dropout_p=0):\n",
    "        B, Nt, E = q.shape\n",
    "        q = q / math.sqrt(E)\n",
    "        # (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\n",
    "        attn = torch.bmm(q, k.transpose(-2, -1))\n",
    "        if attn_mask is not None:\n",
    "            attn += attn_mask\n",
    "        attn = torch.nn.functional.softmax(attn, dim=-1)\n",
    "        if dropout_p > 0.0:\n",
    "            attn = torch.nn.functional.dropout(attn, p=dropout_p)\n",
    "        # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\n",
    "        output = torch.bmm(attn, v)\n",
    "        return output, attn\n",
    "\n",
    "    def cross_attention(self, spatials, freqs):\n",
    "        \"\"\"\n",
    "            spatials: (B, N, D) --> Query,\n",
    "            freqs: (B, N, D) --> Key\n",
    "            output: \n",
    "        \"\"\"\n",
    "        emb_dim = spatials.shape[2]\n",
    "        assert emb_dim == freqs.shape[2]\n",
    "        attn_outputs, attn_weights = self.scale_dot(spatials, freqs, freqs)\n",
    "        return attn_outputs, attn_weights\n",
    "\n",
    "\n",
    "    def forward(self, spatial_imgs, frequency_imgs):\n",
    "        p = self.patch_size\n",
    "        # Extract features\n",
    "        spatial_features = self.spatial_extractor.extract_features(spatial_imgs)                 # shape (batchsize, 1280, 8, 8)\n",
    "        freq_features = self.freq_extractor.extract_features(frequency_imgs)                     # shape (batchsize, 1280, 8, 8)\n",
    "        # print(\"Features shape: \", spatial_features.shape, freq_features.shape)\n",
    "\n",
    "        # Flatten to vector:\n",
    "        spatial_vectors = rearrange(spatial_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        freq_vectors = rearrange(freq_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "\n",
    "        assert self.patch_dim == spatial_vectors.shape[2]\n",
    "        assert self.patch_dim == freq_vectors.shape[2]\n",
    "\n",
    "        embed = None\n",
    "        \n",
    "        if \"cross_attention\" in self.version:          # Merge using cross-attention  \n",
    "            ########## Patch embedding and add position embedding to each domain:\n",
    "            attn_outputs, attn_weights = self.cross_attention(spatial_vectors, freq_vectors)     # Shape: (), (batchsize, num_patches, num_patches)\n",
    "            out_attn = torch.bmm(attn_weights, freq_vectors)\n",
    "\n",
    "            # Concat or add and linear\n",
    "            # print(\"Spatial vectors: \", spatial_vectors.shape)\n",
    "            # print(spatial_vectors)\n",
    "            # print(\"Output attention: \", out_attn.shape)\n",
    "            # print(out_attn)\n",
    "            if \"add\" in self.version:\n",
    "                out = torch.add(spatial_vectors, self.weight * out_attn)\n",
    "                # print(\"Out\", out)\n",
    "                embed = self.patch_to_embedding_add(out)                 # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            elif \"cat\" in self.version:\n",
    "                out = torch.cat([spatial_vectors, self.weight * out_attn], dim=2)\n",
    "                embed = self.patch_to_embedding_cat(out)                 # Shape: (batchsize, num_patches, 2*patch_dim) => (batchsize, num_patches, dim)\n",
    "            else:\n",
    "                pass\n",
    "        else:   # Merge directly\n",
    "            if \"add\" in self.version:\n",
    "                out = torch.add(spatial_vectors, self.weight * freq_vectors)\n",
    "                embed = self.patch_to_embedding_add(out)                # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            elif \"cat\" in self.version:\n",
    "                out = torch.cat([spatial_vectors, self.weight * freq_vectors], dim=2)\n",
    "                embed = self.patch_to_embedding_cat(out)                # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        # print(\"Embeded shape: \", embed.shape)\n",
    "\n",
    "        # Expand classify token to batchsize and add to patch embeddings:\n",
    "        cls_tokens = self.cls_token.expand(embed.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, embed), dim=1)   # (batchsize, num_patches+1, dim)\n",
    "        x += self.pos_embedding\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        x = self.mlp_head(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x = torch.ones(32, 3, 256, 256)\n",
    "    y = torch.ones(32, 1, 256, 256)\n",
    "    model_ = DualEfficientViTV2(image_size=256, patch_size=2)\n",
    "    out = model_(x, y)\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from einops import rearrange\n",
    "\n",
    "import sys\n",
    "from model.backbone.efficient_net.model import EfficientNet\n",
    "\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import re, math\n",
    "from model.vision_transformer.vit import Transformer\n",
    "\n",
    "\n",
    "class DualCrossAttnEfficient(nn.Module):\n",
    "    def __init__(self, channels=1280,\\\n",
    "                 image_size=224,patch_size=7,num_classes=1,dim=1024,\\\n",
    "                 depth=6,heads=8,mlp_dim=2048,\\\n",
    "                 emb_dim=32, dim_head=64,dropout=0.15,emb_dropout=0.15,version=\"cross_attention-spatial-cat\",weight=0.5,freeze=0):  \n",
    "        super(DualCrossAttnEfficient, self).__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.heads = heads\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dim_head = dim_head\n",
    "        self.dropout_value = dropout\n",
    "        self.emb_dropout = emb_dropout\n",
    "        self.features_size = {\n",
    "            128: (4, 4),\n",
    "            224: (7, 7),\n",
    "            256: (8, 8)\n",
    "        }\n",
    "        \n",
    "        # \"cross_attention-spatial-cat\": sử dụng cross-attention, cat với spatial vectors output\n",
    "        # \"cross_attention-spatial-add\": sử dụng cross-attention, add với spatial vectors output\n",
    "        # \"cross_attention-freq-cat\": sử dụng cross-attention, cat với freq vectors\n",
    "        # \"cross_attention-freq-add\": sử dụng cross-attention, add với freq vectors\n",
    "        # \"merge-add\": cộng thẳng 2 vectors spatial và freq, có weight: spatial + weight*freq\n",
    "        # \"merge-cat\": cat thẳng 2 vectors spatial và freq, có weight: spatial + weight*freq\n",
    "        self.version = version\n",
    "        self.weight = weight\n",
    "\n",
    "        self.spatial_extractor = self.get_feature_extractor(freeze=freeze, num_classes=num_classes, in_channels=3)   # efficient_net-b0, return shape (1280, 8, 8) or (1280, 7, 7)\n",
    "        self.freq_extractor = self.get_feature_extractor(freeze=freeze, num_classes=num_classes, in_channels=1)\n",
    "\n",
    "        ############################# Xét 2 stream hiện tại là như nhau\n",
    "        # Kích thước của 1 patch\n",
    "        self.patch_size = patch_size\n",
    "    \n",
    "        # Số lượng patches\n",
    "        self.num_patches = int((self.features_size[image_size][0] * self.features_size[image_size][1]) / (self.patch_size * self.patch_size))\n",
    "        # Patch_dim = P^2 * C\n",
    "        self.patch_dim = channels * (self.patch_size ** 2)\n",
    "\n",
    "        # print(\"Num patches: \", self.num_patches)\n",
    "        # print(\"Patch dim: \", self.patch_dim)\n",
    "\n",
    "        # Embed vị trí cho từng patch\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches+1, self.dim))\n",
    "        # self.pos_embedding_1 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "        # self.pos_embedding_2 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "        # self.pos_embedding_3 = nn.Parameter(torch.randn(1, self.num_patches, self.dim))\n",
    "\n",
    "        # Đưa flatten vector của feature maps về chiều cố định của vector trong transformer.\n",
    "        # self.patch_to_embedding_1 = nn.Linear(self.patch_dim, self.dim)\n",
    "        # self.patch_to_embedding_2 = nn.Linear(self.patch_dim, self.dim)\n",
    "\n",
    "        # Giảm chiều vector sau concat 2*patch_dim về D:\n",
    "        self.patch_to_embedding_cat = nn.Linear(2*self.patch_dim, self.dim)\n",
    "        self.patch_to_embedding_add = nn.Linear(self.patch_dim, self.dim)\n",
    "\n",
    "        # Thêm 1 embedding vector cho classify token:\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(self.emb_dropout)\n",
    "        self.transformer = Transformer(self.dim, self.depth, self.heads, self.dim_head, self.mlp_dim, self.dropout_value)\n",
    "\n",
    "        self.to_cls_token = nn.Identity()\n",
    "        \n",
    "        self.reduce_dim = nn.Linear(self.num_patches, 1)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(self.dim, self.mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.mlp_dim, self.num_classes)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def get_feature_extractor(self, architecture=\"efficient_net\", freeze=0, pretrained=\"\", num_classes=1, in_channels=3):\n",
    "        extractor = None\n",
    "        if architecture == \"efficient_net\":\n",
    "            if pretrained == \"\":\n",
    "                extractor = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes,in_channels = in_channels)\n",
    "            else:\n",
    "                extractor = EfficientNet.from_pretrained('efficientnet-b7', num_classes=num_classes,in_channels = in_channels)\n",
    "                # Load checkpoint\n",
    "                checkpoint = torch.load(pretrained, map_location=\"cpu\")\n",
    "                state_dict = checkpoint.get(\"state_dict\", checkpoint)\n",
    "                # Load weights\n",
    "                extractor.load_state_dict({re.sub(\"^module.\", \"\", k): v for k, v in state_dict.items()}, strict=False)\n",
    "\n",
    "            if freeze:\n",
    "            # Freeze the first (num_blocks - 3) blocks and unfreeze the rest \n",
    "                for i in range(0, len(extractor._blocks)):\n",
    "                    for index, param in enumerate(extractor._blocks[i].parameters()):\n",
    "                        if i >= len(extractor._blocks) - 3:\n",
    "                            param.requires_grad = True\n",
    "                        else:\n",
    "                            param.requires_grad = False\n",
    "        return extractor\n",
    "\n",
    "    \"\"\"\n",
    "        Get from torch.nn.MultiheadAttention\n",
    "        scale-dot: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L4966\n",
    "        multi-head: https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/functional.py#L5059\n",
    "    \"\"\"\n",
    "    def scale_dot(self, q, k, v, attn_mask=None, dropout_p=0):\n",
    "        B, Nt, E = q.shape\n",
    "        q = q / math.sqrt(E)\n",
    "        # (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\n",
    "        attn = torch.bmm(q, k.transpose(-2, -1))\n",
    "        if attn_mask is not None:\n",
    "            attn += attn_mask\n",
    "        attn = torch.nn.functional.softmax(attn, dim=-1)\n",
    "        if dropout_p > 0.0:\n",
    "            attn = torch.nn.functional.dropout(attn, p=dropout_p)\n",
    "        # (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\n",
    "        output = torch.bmm(attn, v)\n",
    "        return output, attn\n",
    "\n",
    "    def cross_attention(self, spatials, ifreqs):\n",
    "        \"\"\"\n",
    "            spatials: (B, N, D) --> Query,\n",
    "            freqs: (B, N, D) --> Key\n",
    "            output: \n",
    "        \"\"\"\n",
    "        emb_dim = spatials.shape[2]\n",
    "        assert emb_dim == ifreqs.shape[2]\n",
    "        attn_outputs, attn_weights = self.scale_dot(spatials, ifreqs, ifreqs)\n",
    "        return attn_outputs, attn_weights\n",
    "\n",
    "\n",
    "    def forward(self, spatial_imgs, frequency_imgs):\n",
    "        p = self.patch_size\n",
    "        # Extract features\n",
    "        spatial_features = self.spatial_extractor.extract_features(spatial_imgs)                 # shape (batchsize, 1280, 8, 8)\n",
    "        freq_features = self.freq_extractor.extract_features(frequency_imgs)                     # shape (batchsize, 1280, 8, 8)conda\n",
    "        ifreq_features = torch.log(torch.abs(torch.fft.ifft2(torch.fft.ifftshift(freq_features))) + 1e-10)  # Hơi ảo???\n",
    "        # print(ifreq_features.shape)\n",
    "        # assert(ifreq_features.shape == freq_features.shape)\n",
    "        # print(\"Features shape: \", spatial_features.shape, freq_features.shape)\n",
    "\n",
    "        # Flatten to vector:\n",
    "        spatial_vectors = rearrange(spatial_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        freq_vectors = rearrange(freq_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "        ifreq_vectors = rearrange(ifreq_features, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n",
    "\n",
    "        assert self.patch_dim == spatial_vectors.shape[2]\n",
    "        assert self.patch_dim == freq_vectors.shape[2]\n",
    "\n",
    "        embed = None\n",
    "        \n",
    "        if \"cross_attention\" in self.version:          # Merge using cross-attention  \n",
    "            ########## Patch embedding and add position embedding to each domain:\n",
    "            # spatial_vectors = self.patch_to_embedding_1(spatial_vectors)\n",
    "            # spatial_vectors += self.pos_embedding_1\n",
    "\n",
    "            # freq_vectors = self.patch_to_embedding_2(freq_vectors)\n",
    "            # freq_vectors += self.pos_embedding_2\n",
    "\n",
    "            # ifreq_vectors = self.patch_to_embedding_2(ifreq_vectors)\n",
    "            # ifreq_vectors += self.pos_embedding_2  \n",
    "            # print(\"Step 2 shape: \", spatial_vectors.shape, freq_vectors.shape)  # (batchsize, num_patches, D)\n",
    "            ##########\n",
    "        \n",
    "            # Cal attn weight between ifreq and spatial vectors:\n",
    "            # Cross-attention (spatial-decoder, ifreq-encoder)\n",
    "            attn_outputs, attn_weights = self.cross_attention(spatial_vectors, ifreq_vectors)     # Shape: (), (batchsize, num_patches, num_patches)\n",
    "            if \"freq\" in self.version:          # Get attention in frequency domain:\n",
    "                out_attn = torch.bmm(attn_weights, freq_vectors)\n",
    "            elif \"spatial\" in self.version:     # Get attention in spatial domain:\n",
    "                out_attn = torch.bmm(attn_weights, ifreq_vectors)\n",
    "                ### Check correct bmm:\n",
    "                # print(torch.eq(attn_outputs, out_attn))\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # Concat or add and linear\n",
    "            # print(\"Spatial vectors: \", spatial_vectors.shape)\n",
    "            # print(spatial_vectors)\n",
    "            # print(\"Output attention: \", out_attn.shape)\n",
    "            # print(out_attn)\n",
    "            if \"add\" in self.version:\n",
    "                out = torch.add(spatial_vectors, self.weight * out_attn)\n",
    "                # print(\"Out\", out)\n",
    "                embed = self.patch_to_embedding_add(out)                 # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            elif \"cat\" in self.version:\n",
    "                out = torch.cat([spatial_vectors, self.weight * out_attn], dim=2)\n",
    "                embed = self.patch_to_embedding_cat(out)                 # Shape: (batchsize, num_patches, 2*patch_dim) => (batchsize, num_patches, dim)\n",
    "            else:\n",
    "                pass\n",
    "        else:   # Merge directly\n",
    "            if \"add\" in self.version:\n",
    "                out = torch.add(spatial_vectors, self.weight * freq_vectors)\n",
    "                embed = self.patch_to_embedding_add(out)                # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            elif \"cat\" in self.version:\n",
    "                out = torch.cat([spatial_vectors, self.weight * freq_vectors], dim=2)\n",
    "                embed = self.patch_to_embedding_cat(out)                # Shape: (batchsize, num_patches, patch_dim) => (batchsize, num_patches, dim)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        # print(\"Embeded shape: \", embed.shape)\n",
    "\n",
    "        # Expand classify token to batchsize and add to patch embeddings:\n",
    "        print(\"Embed:\", embed.shape)\n",
    "        embed = embed.permute(0, 2, 1)\n",
    "        x = self.reduce_dim(embed)\n",
    "        print(\"Before squeeze:\", x.shape)\n",
    "        x = x.squeeze(-1)\n",
    "        print(\"Before mlp:\", x.shape)\n",
    "        x = self.mlp_head(x)\n",
    "        print(\"After mlp: \", x.shape)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x = torch.ones(32, 3, 256, 256)\n",
    "    y = torch.ones(32, 1, 256, 256)\n",
    "    model_ = DualCrossAttnEfficient(image_size=256, patch_size=2)\n",
    "    device = torch.device(\"cpu\")\n",
    "    import os\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "    model_.to(device)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    out = model_(x, y)\n",
    "    print(out.shape)\n",
    "    \n",
    "    from torchsummary import summary\n",
    "    summary(model_, [(3, 256, 256), (1, 256, 256)], device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.cnn.srm_2_stream_net.srm_2_stream import Two_Stream_Net\n",
    "import torch\n",
    "\n",
    "model = Two_Stream_Net(num_classes=1, pretrained=\"/mnt/disk1/phucnp/Graduation_Thesis/review/forensics/dl_technique/xception-b5690688.pth\")\n",
    "dummy = torch.rand((32,3,128,128))\n",
    "out = model(dummy)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_result import plot_graph\n",
    "\n",
    "for iter_method in ['epoch', 'step']:\n",
    "    print(iter_method)\n",
    "    plot_graph(\"/mnt/disk1/phucnp/Graduation_Thesis/review/forensics/dl_technique/checkpoint/uadfv/dual_cross_attn/v_cross_attention-freq-add_w_0.8_lr_128_patch_0.0003_es_2_loss_none_freeze_bce/{}/result_test.csv\".format(iter_method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loaded pretrained weights for efficientnet-b0\n",
      "Test (dual) dataset:  117662\n",
      "Test dataset:  117662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1839/1839 [01:35<00:00, 19.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom :   0.4930478829188693 0.4956455954538257 0.41892570281124497 0.4930478829188693 0.45728564913253533 0.9605346153190474 0.02828669728657865 0.4930478829188693 0.494410656302813 0.6538816388819579 0.05299506247320876 0.4930478829188693 0.35343835067758334\n",
      "Sklearn :  0.4930478829188693 0.4956455954538257 0.41892570281124497 0.4930478829188693 0.45728564913253533 0.9605346153190474 0.02828669728657865 0.4930478829188693 0.494410656302813 0.6538816388819579 0.05299506247320876 0.4930478829188693 0.35343835067758334\n",
      "Micro:  0.4930478829188693 0.4930478829188693 0.4930478829188693\n",
      "\n",
      "Test (dual) dataset:  117662\n",
      "Test dataset:  117662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1839/1839 [01:36<00:00, 19.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom :   0.5058897520014958 0.5023115898946696 0.5870396458039847 0.5058897520014958 0.5446756178493272 0.9650181557817215 0.04943816416114435 0.5058897520014958 0.5072281599714329 0.6607101171856763 0.09119614831488777 0.5058897520014958 0.375953132750282\n",
      "Sklearn :  0.5058897520014958 0.5023115898946696 0.5870396458039847 0.5058897520014958 0.5446756178493272 0.9650181557817215 0.04943816416114435 0.5058897520014958 0.5072281599714329 0.6607101171856763 0.09119614831488777 0.5058897520014958 0.375953132750282\n",
      "Micro:  0.5058897520014958 0.5058897520014958 0.5058897520014958\n",
      "\n",
      "Test (dual) dataset:  117662\n",
      "Test dataset:  117662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1839/1839 [01:34<00:00, 19.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom :   0.5134792881304074 0.5067128750474743 0.5712202236266407 0.5134792881304074 0.5389665493370575 0.9097836649107554 0.11948544989237836 0.5134792881304074 0.5146345574015668 0.6509004201757541 0.19763122853738874 0.5134792881304074 0.42426582435657145\n",
      "Sklearn :  0.5134792881304074 0.5067128750474743 0.5712202236266407 0.5134792881304074 0.5389665493370575 0.9097836649107554 0.11948544989237836 0.5134792881304074 0.5146345574015668 0.6509004201757541 0.19763122853738874 0.5134792881304074 0.42426582435657145\n",
      "Micro:  0.5134792881304074 0.5134792881304074 0.5134792881304074\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from metrics.metric import *\n",
    "from sklearn import metrics\n",
    "from dataloader.gen_dataloader import *\n",
    "from tqdm import tqdm\n",
    "from model.vision_transformer.dual_efficient_vit import DualEfficientViT\n",
    "import time\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "def eval_dual_stream(model, criterion, test_dir='',image_size=256, \\\n",
    "            batch_size=16, num_workers=8, checkpoint=\"checkpoint\", resume=\"\", \\\n",
    "            adj_brightness=1.0, adj_contrast=1.0, show_time=False, model_name=\"\", args_txt=\"\"):\n",
    "\n",
    "      # Defines device and push model and loss module to device\n",
    "      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "      model = model.to(device)\n",
    "      criterion = criterion.to(device)\n",
    "\n",
    "      # Load trained model\n",
    "      try:\n",
    "            model.load_state_dict(torch.load(osp.join(checkpoint, resume), map_location=device))\n",
    "      except:\n",
    "            print(\"ERROR !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            print(\"model khong ton tai: \", osp.join(checkpoint, resume) + '.pt')\n",
    "            return\n",
    "\n",
    "      # Make dataloader\n",
    "      dataloader_test = generate_test_dataloader_dual_stream(test_dir, image_size, batch_size, num_workers, adj_brightness=adj_brightness, adj_contrast=adj_contrast)\n",
    "\n",
    "      test_loss = 0.0           # global loss of all images\n",
    "      test_accuracy = 0.0       # global accuracy of all images\n",
    "      model.eval()\n",
    "      \n",
    "      y_label = []              # save ground truth labels  [0, 0, 1, 1...]           - (num_images, ) \n",
    "      y_pred = []               # save output of all images [0.05, 0.4, 0.6, 0.4...]  - (num_images, )\n",
    "      y_pred_label = []         # save predicted labels     [0, 0, 1, 0...]           - (num_images, )\n",
    "      \n",
    "      with torch.no_grad():\n",
    "            for inputs, fft_imgs, labels in tqdm(dataloader_test):\n",
    "                  begin = time.time()\n",
    "                  # Push groundtruth label (original) to cpu and save in a list\n",
    "                  y_label.extend(labels.cpu().numpy().astype(np.float64))\n",
    "                  # Push input to device and forward to model\n",
    "                  inputs, fft_imgs, labels = inputs.float().to(device), fft_imgs.float().to(device), labels.float().to(device)\n",
    "                  logps = model.forward(inputs, fft_imgs)      # shape (batchsize, 1)\n",
    "                  logps = logps.squeeze()                      # shape (batchsize, )   \n",
    "                  \n",
    "                  # Push predicted output - in [0, 1] to cpu and save in a list\n",
    "                  logps_cpu = logps.cpu().numpy()\n",
    "                  y_pred.extend(logps_cpu.astype(np.float64))\n",
    "                  # breakpoint()\n",
    "                  \n",
    "                  # Show batch processing time\n",
    "                  if show_time:\n",
    "                        print(\"Time : \", time.time() - begin)\n",
    "                  \n",
    "                  # Calculate loss\n",
    "                  batch_loss = criterion(logps, labels)\n",
    "                  test_loss += batch_loss.item()\n",
    "\n",
    "                  # Find predicted label, 0 or 1 and save in a list\n",
    "                  equals = labels == (logps > 0.5)\n",
    "                  pred_label = (logps_cpu > 0.5)\n",
    "                  y_pred_label.extend(pred_label)\n",
    "                  \n",
    "                  # Calculate mean accuracy\n",
    "                  test_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "                  \n",
    "      # breakpoint()\n",
    "      # import pdb; pdb.set_trace()\n",
    "      # from IPython.core.debugger import Tracer; Tracer()()\n",
    "      # from IPython.core.debugger import set_trace; set_trace()\n",
    "      ######## Calculate metrics:\n",
    "      test_loss /= len(dataloader_test)\n",
    "      test_accuracy /= len(dataloader_test)\n",
    "      # built-in methods for calculating metrics\n",
    "      y_label = np.array(y_label, dtype=np.float64)\n",
    "      y_pred_label = np.array(y_pred_label, dtype=np.float64)\n",
    "      \n",
    "      custom_accuracy = accuracy_score(y_label, y_pred_label)\n",
    "      sklearn_acc = metrics.accuracy_score(y_true=y_label, y_pred=y_pred_label)\n",
    "      \n",
    "      custom_pre_0 = precision_score(y_label, y_pred_label, average='binary', pos_label=0)\n",
    "      sklearn_pre_0 = metrics.precision_score(y_label, y_pred_label, average='binary', pos_label=0)\n",
    "      \n",
    "      custom_pre_1 = precision_score(y_label, y_pred_label, average='binary', pos_label=1)\n",
    "      sklearn_pre_1 = metrics.precision_score(y_label, y_pred_label, average='binary', pos_label=1)\n",
    "      \n",
    "      custom_pre_micro = precision_score(y_label, y_pred_label, average='micro')\n",
    "      sklearn_pre_micro = metrics.precision_score(y_label, y_pred_label, average='micro')\n",
    "\n",
    "      custom_pre_macro = precision_score(y_label, y_pred_label, average='macro')\n",
    "      sklearn_pre_macro = metrics.precision_score(y_label, y_pred_label, average='macro')\n",
    "      \n",
    "      custom_rec_0 = recall_score(y_label, y_pred_label, average='binary', pos_label=0)\n",
    "      sklearn_rec_0 = metrics.recall_score(y_label, y_pred_label, average='binary', pos_label=0)\n",
    "      \n",
    "      custom_rec_1 = recall_score(y_label, y_pred_label, average='binary', pos_label=1)\n",
    "      sklearn_rec_1 = metrics.recall_score(y_label, y_pred_label, average='binary', pos_label=1)\n",
    "      \n",
    "      custom_rec_micro = recall_score(y_label, y_pred_label, average='micro')\n",
    "      sklearn_rec_micro = metrics.recall_score(y_label, y_pred_label, average='micro')\n",
    "\n",
    "      custom_rec_macro = recall_score(y_label, y_pred_label, average='macro')\n",
    "      sklearn_rec_macro = metrics.recall_score(y_label, y_pred_label, average='macro')\n",
    "      \n",
    "      custom_f1_0 = f1_score(y_label, y_pred_label, average='binary', pos_label=0)\n",
    "      sklearn_f1_0 = metrics.f1_score(y_label, y_pred_label, average='binary', pos_label=0)\n",
    "      \n",
    "      custom_f1_1 = f1_score(y_label, y_pred_label, average='binary', pos_label=1)\n",
    "      sklearn_f1_1 = metrics.f1_score(y_label, y_pred_label, average='binary', pos_label=1)\n",
    "      \n",
    "      custom_f1_micro = f1_score(y_label, y_pred_label, average='micro')\n",
    "      sklearn_f1_micro = metrics.f1_score(y_label, y_pred_label, average='micro')\n",
    "\n",
    "      custom_f1_macro = f1_score(y_label, y_pred_label, average='macro')\n",
    "      sklearn_f1_macro = metrics.f1_score(y_label, y_pred_label, average='macro')\n",
    "      \n",
    "      print(\"Custom :  \", custom_accuracy, custom_pre_0, custom_pre_1, custom_pre_micro, custom_pre_macro, custom_rec_0, custom_rec_1, custom_rec_micro, custom_rec_macro, custom_f1_0, custom_f1_1, custom_f1_micro, custom_f1_macro)\n",
    "      print(\"Sklearn : \", sklearn_acc, sklearn_pre_0, sklearn_pre_1, sklearn_pre_micro, sklearn_pre_macro, sklearn_rec_0, sklearn_rec_1, sklearn_rec_micro, sklearn_rec_macro, sklearn_f1_0, sklearn_f1_1, sklearn_f1_micro, sklearn_f1_macro)\n",
    "      print(\"Micro: \", custom_pre_micro, custom_rec_micro, custom_f1_micro)\n",
    "      assert custom_f1_micro == custom_pre_micro, \"1\"\n",
    "      assert custom_f1_micro == custom_rec_micro, \"2\"\n",
    "\n",
    "model = DualEfficientViT(image_size=128, patch_size=2, heads=3, depth=4, version=\"cross_attentio-freq-add\",weight=0.8, freeze=0)\n",
    "checkpoint='/mnt/disk1/phucnp/Graduation_Thesis/review/forensics/dl_technique/checkpoint/uadfv/dual_efficient_vit/(0.95_0.95_0.95)_v_cross_attention-freq-add_w_0.8_lr_0.0003_patch_2_h_3_d_4_es_none_loss_bce_freeze_0/epoch'\n",
    "resumes=['model_last_30.pt', 'best_test_acc_0.947195.pt', 'best_test_loss_0.136948.pt']\n",
    "criterion = nn.BCELoss()\n",
    "test_dir = '/mnt/disk1/phucnp/Dataset/df_in_the_wild/image/test'\n",
    "for resume in resumes:\n",
    "      eval_dual_stream(model, criterion, test_dir, image_size=128, batch_size=64, num_workers=4, checkpoint=checkpoint, resume=resume)\n",
    "      print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "y_label =       np.array([0, 1, 0, 1, 0, 1, 1, 1, 1, 1], dtype=np.float64)\n",
    "y_pred_label =  np.array([1, 1, 1, 1, 0, 1, 0, 0, 0, 0], dtype=np.float64)\n",
    "\n",
    "label = 0\n",
    "tp_1 = np.sum((y_label == label) * (y_pred_label == label))\n",
    "label = 1\n",
    "tp_2 = np.sum((y_label == label) * (y_pred_label == label))\n",
    "print(tp_1)\n",
    "print(tp_2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c99a311309def2227702d5f2c551c7a298c8ef3054a27c10387f667008a8451"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('phucnp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
